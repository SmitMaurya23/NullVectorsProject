{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwhfk3Rk5tcJ",
        "outputId": "604198ef-cd28-4dd3-af88-9ebba7cdb706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'allmrec_CrossModal'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 89 (delta 16), reused 68 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (89/89), 19.04 MiB | 19.61 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SmitMaurya23/allmrec_CrossModal.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyWTMNlo51uy",
        "outputId": "ceac7aaf-6a06-4896-a0d0-498c71e3fa48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/allmrec_CrossModal\n"
          ]
        }
      ],
      "source": [
        "%cd allmrec_CrossModal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eknrWcmPja9",
        "outputId": "91b988cf-5b8c-42d0-e0ad-6722ed4b7495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/allmrec_CrossModal/A-LLMRec\n"
          ]
        }
      ],
      "source": [
        "%cd A-LLMRec/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_fHko3VPuMk",
        "outputId": "e23e4de0-84a6-470e-d57b-aa884444918b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/allmrec_CrossModal/A-LLMRec/data/amazon\n"
          ]
        }
      ],
      "source": [
        "%cd data/amazon/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HjHj_9pz5_iT"
      },
      "outputs": [],
      "source": [
        "\n",
        "!gunzip  meta_Movies_and_TV.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AKm1yWNf-0Ha"
      },
      "outputs": [],
      "source": [
        "!mv Movies_and_TV.json.gz meta_Movies_and_TV.json allmrec_CrossModal/A-LLMRec/data/amazon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc3zF3HyAQnu",
        "outputId": "d504c3b6-b867-4752-c1db-e0a8ebcdc396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.66.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2024.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.1.1)\n",
            "Collecting bitsandbytes (from -r requirements.txt (line 6))\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (4.46.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 5)) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 5)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 5)) (0.4.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (0.20.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 8)) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 8)) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 8)) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 7)) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 8)) (3.5.0)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FwHKadY-_i6",
        "outputId": "86feadc4-41bf-44d7-c7bb-a7788c6bee39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/allmrec_CrossModal/A-LLMRec/pre_train/sasrec\n"
          ]
        }
      ],
      "source": [
        "%cd pre_train/sasrec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Ug9xC__Mds",
        "outputId": "eacfea85-b83f-4c6b-e1f8-351266b1d0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000it [00:00, 83049.82it/s]\n",
            "30000it [00:00, 87987.22it/s]\n",
            "73 67\n",
            "user num: 73 item num: 67\n",
            "average sequence length: 4.47\n",
            "  0% 0/4000 [00:00<?, ?it/s]Evaluating\n",
            "\n",
            "epoch:1, time: 0.000486(s), valid (NDCG@10: 0.1267, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12670961255519897, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            "  0% 1/4000 [00:03<3:41:38,  3.33s/it]Evaluating\n",
            "\n",
            "epoch:20, time: 0.001144(s), valid (NDCG@10: 0.1316, HR@10: 0.1781), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.13161475866832698, 0.1780821917808219) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            "  0% 20/4000 [00:04<11:18,  5.87it/s] Evaluating\n",
            "\n",
            "epoch:40, time: 0.001846(s), valid (NDCG@10: 0.1256, HR@10: 0.1644), test (NDCG@10: 0.0822, HR@10: 0.0822)\n",
            "(0.12559211609065268, 0.1643835616438356) (0.0821917808219178, 0.0821917808219178)\n",
            "\n",
            "  1% 40/4000 [00:04<05:34, 11.85it/s]Evaluating\n",
            "\n",
            "epoch:60, time: 0.002290(s), valid (NDCG@10: 0.1267, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12667578088703935, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            "  2% 60/4000 [00:05<03:42, 17.71it/s]Evaluating\n",
            "\n",
            "epoch:80, time: 0.002974(s), valid (NDCG@10: 0.1283, HR@10: 0.1644), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.1282712388554925, 0.1643835616438356) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            "  2% 80/4000 [00:05<02:50, 23.03it/s]Evaluating\n",
            "\n",
            "epoch:100, time: 0.003444(s), valid (NDCG@10: 0.1309, HR@10: 0.1781), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.1309286504700345, 0.1780821917808219) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            "  2% 100/4000 [00:06<02:19, 27.91it/s]Evaluating\n",
            "\n",
            "epoch:120, time: 0.003910(s), valid (NDCG@10: 0.1269, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12693167747307257, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            "  3% 120/4000 [00:06<02:01, 31.88it/s]Evaluating\n",
            "\n",
            "epoch:140, time: 0.004369(s), valid (NDCG@10: 0.1232, HR@10: 0.1507), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12315850022062104, 0.1506849315068493) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            "  4% 140/4000 [00:07<01:50, 35.08it/s]Evaluating\n",
            "\n",
            "epoch:160, time: 0.004828(s), valid (NDCG@10: 0.1181, HR@10: 0.1370), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.11811504328289343, 0.136986301369863) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            "  4% 160/4000 [00:07<01:42, 37.32it/s]Evaluating\n",
            "\n",
            "epoch:180, time: 0.005271(s), valid (NDCG@10: 0.1187, HR@10: 0.1370), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1187214611872146, 0.136986301369863) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            "  4% 180/4000 [00:08<01:37, 39.04it/s]Evaluating\n",
            "\n",
            "epoch:200, time: 0.005905(s), valid (NDCG@10: 0.1264, HR@10: 0.1644), test (NDCG@10: 0.0548, HR@10: 0.0548)\n",
            "(0.1263624404243724, 0.1643835616438356) (0.0547945205479452, 0.0547945205479452)\n",
            "\n",
            "  5% 200/4000 [00:08<01:34, 40.25it/s]Evaluating\n",
            "\n",
            "epoch:220, time: 0.006354(s), valid (NDCG@10: 0.1269, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12692182304888727, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            "  6% 220/4000 [00:09<01:32, 40.96it/s]Evaluating\n",
            "\n",
            "epoch:240, time: 0.006788(s), valid (NDCG@10: 0.1240, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12401472216061113, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            "  6% 240/4000 [00:09<01:29, 42.00it/s]Evaluating\n",
            "\n",
            "epoch:260, time: 0.007228(s), valid (NDCG@10: 0.1270, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12700268999685319, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            "  6% 260/4000 [00:09<01:27, 42.61it/s]Evaluating\n",
            "\n",
            "epoch:280, time: 0.007725(s), valid (NDCG@10: 0.1222, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.1222387418536329, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            "  7% 280/4000 [00:10<01:25, 43.28it/s]Evaluating\n",
            "\n",
            "epoch:300, time: 0.008185(s), valid (NDCG@10: 0.1256, HR@10: 0.1644), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12559211609065268, 0.1643835616438356) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            "  8% 300/4000 [00:10<01:24, 43.79it/s]Evaluating\n",
            "\n",
            "epoch:320, time: 0.008620(s), valid (NDCG@10: 0.1193, HR@10: 0.1370), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.11934814211254857, 0.136986301369863) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            "  8% 320/4000 [00:11<01:23, 44.17it/s]Evaluating\n",
            "\n",
            "epoch:340, time: 0.009044(s), valid (NDCG@10: 0.1226, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12260038638058962, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            "  8% 340/4000 [00:11<01:23, 43.78it/s]Evaluating\n",
            "\n",
            "epoch:360, time: 0.009481(s), valid (NDCG@10: 0.1186, HR@10: 0.1370), test (NDCG@10: 0.0822, HR@10: 0.0822)\n",
            "(0.11859229017495895, 0.136986301369863) (0.0821917808219178, 0.0821917808219178)\n",
            "\n",
            "  9% 360/4000 [00:12<01:22, 43.99it/s]Evaluating\n",
            "\n",
            "epoch:380, time: 0.009934(s), valid (NDCG@10: 0.1325, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13245985739964825, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 10% 380/4000 [00:12<01:22, 43.63it/s]Evaluating\n",
            "\n",
            "epoch:400, time: 0.010384(s), valid (NDCG@10: 0.1263, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.1263154051445661, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 10% 400/4000 [00:13<01:22, 43.68it/s]Evaluating\n",
            "\n",
            "epoch:420, time: 0.010800(s), valid (NDCG@10: 0.1262, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12619853399497385, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 10% 420/4000 [00:13<01:22, 43.20it/s]Evaluating\n",
            "\n",
            "epoch:440, time: 0.011273(s), valid (NDCG@10: 0.1308, HR@10: 0.1781), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.13076474404063593, 0.1780821917808219) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 11% 440/4000 [00:14<01:22, 43.15it/s]Evaluating\n",
            "\n",
            "epoch:460, time: 0.011728(s), valid (NDCG@10: 0.1243, HR@10: 0.1507), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12432806262327813, 0.1506849315068493) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 12% 460/4000 [00:14<01:30, 39.12it/s]Evaluating\n",
            "\n",
            "epoch:480, time: 0.012368(s), valid (NDCG@10: 0.1355, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13554711842435868, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 12% 480/4000 [00:15<01:36, 36.42it/s]Evaluating\n",
            "\n",
            "epoch:500, time: 0.012982(s), valid (NDCG@10: 0.1222, HR@10: 0.1507), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12219170657382664, 0.1506849315068493) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 12% 500/4000 [00:15<01:43, 33.91it/s]Evaluating\n",
            "\n",
            "epoch:520, time: 0.013669(s), valid (NDCG@10: 0.1283, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12833615882890878, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 13% 520/4000 [00:16<01:45, 32.95it/s]Evaluating\n",
            "\n",
            "epoch:540, time: 0.014146(s), valid (NDCG@10: 0.1381, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1380565705840517, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 14% 540/4000 [00:17<01:37, 35.40it/s]Evaluating\n",
            "\n",
            "epoch:560, time: 0.014573(s), valid (NDCG@10: 0.1237, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12372773726932125, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 14% 560/4000 [00:17<01:33, 36.89it/s]Evaluating\n",
            "\n",
            "epoch:580, time: 0.015020(s), valid (NDCG@10: 0.1299, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12987972109079068, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 14% 580/4000 [00:18<01:28, 38.60it/s]Evaluating\n",
            "\n",
            "epoch:600, time: 0.015448(s), valid (NDCG@10: 0.1220, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12199396847626845, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 15% 600/4000 [00:18<01:25, 40.00it/s]Evaluating\n",
            "\n",
            "epoch:620, time: 0.015891(s), valid (NDCG@10: 0.1218, HR@10: 0.1507), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12179623037871026, 0.1506849315068493) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 16% 620/4000 [00:18<01:22, 41.17it/s]Evaluating\n",
            "\n",
            "epoch:640, time: 0.016378(s), valid (NDCG@10: 0.1243, HR@10: 0.1507), test (NDCG@10: 0.1918, HR@10: 0.1918)\n",
            "(0.12432806262327813, 0.1506849315068493) (0.1917808219178082, 0.1917808219178082)\n",
            "\n",
            " 16% 640/4000 [00:19<01:22, 40.56it/s]Evaluating\n",
            "\n",
            "epoch:660, time: 0.016805(s), valid (NDCG@10: 0.1231, HR@10: 0.1507), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1231357917611302, 0.1506849315068493) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 16% 660/4000 [00:19<01:20, 41.35it/s]Evaluating\n",
            "\n",
            "epoch:680, time: 0.017256(s), valid (NDCG@10: 0.1310, HR@10: 0.1781), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.130997217555337, 0.1780821917808219) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 17% 680/4000 [00:20<01:18, 42.29it/s]Evaluating\n",
            "\n",
            "epoch:700, time: 0.017696(s), valid (NDCG@10: 0.1269, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12687351898459753, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 18% 700/4000 [00:20<01:16, 43.04it/s]Evaluating\n",
            "\n",
            "epoch:720, time: 0.018127(s), valid (NDCG@10: 0.1184, HR@10: 0.1370), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.1184283837455604, 0.136986301369863) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 18% 720/4000 [00:21<01:16, 43.07it/s]Evaluating\n",
            "\n",
            "epoch:740, time: 0.018581(s), valid (NDCG@10: 0.1237, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12373611711508707, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 18% 740/4000 [00:22<01:30, 36.17it/s]Evaluating\n",
            "\n",
            "epoch:760, time: 0.019023(s), valid (NDCG@10: 0.1260, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12595376061760938, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 19% 760/4000 [00:22<01:27, 37.02it/s]Evaluating\n",
            "\n",
            "epoch:780, time: 0.019564(s), valid (NDCG@10: 0.1314, HR@10: 0.1781), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.13143972903025963, 0.1780821917808219) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 20% 780/4000 [00:23<01:24, 38.10it/s]Evaluating\n",
            "\n",
            "epoch:800, time: 0.020003(s), valid (NDCG@10: 0.1235, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12353126795624657, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 20% 800/4000 [00:23<01:20, 39.75it/s]Evaluating\n",
            "\n",
            "epoch:820, time: 0.020456(s), valid (NDCG@10: 0.1235, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12347184068328801, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 20% 820/4000 [00:23<01:17, 41.29it/s]Evaluating\n",
            "\n",
            "epoch:840, time: 0.020928(s), valid (NDCG@10: 0.1265, HR@10: 0.1644), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12647931157396466, 0.1643835616438356) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 21% 840/4000 [00:24<01:15, 41.91it/s]Evaluating\n",
            "\n",
            "epoch:860, time: 0.021420(s), valid (NDCG@10: 0.1232, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12316962342928985, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 22% 860/4000 [00:24<01:13, 42.71it/s]Evaluating\n",
            "\n",
            "epoch:880, time: 0.022064(s), valid (NDCG@10: 0.1145, HR@10: 0.1233), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.11446859160421949, 0.1232876712328767) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 22% 880/4000 [00:25<01:13, 42.62it/s]Evaluating\n",
            "\n",
            "epoch:900, time: 0.022580(s), valid (NDCG@10: 0.1180, HR@10: 0.1370), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.11803417633492753, 0.136986301369863) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 22% 900/4000 [00:25<01:11, 43.10it/s]Evaluating\n",
            "\n",
            "epoch:920, time: 0.023006(s), valid (NDCG@10: 0.1226, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12255208231629987, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 23% 920/4000 [00:26<01:11, 43.16it/s]Evaluating\n",
            "\n",
            "epoch:940, time: 0.023431(s), valid (NDCG@10: 0.1346, HR@10: 0.1781), test (NDCG@10: 0.1781, HR@10: 0.1781)\n",
            "(0.13456280228604506, 0.1780821917808219) (0.1780821917808219, 0.1780821917808219)\n",
            "\n",
            " 24% 940/4000 [00:26<01:12, 42.36it/s]Evaluating\n",
            "\n",
            "epoch:960, time: 0.025412(s), valid (NDCG@10: 0.1281, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12809138545154436, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 24% 960/4000 [00:27<01:17, 39.17it/s]Evaluating\n",
            "\n",
            "epoch:980, time: 0.026135(s), valid (NDCG@10: 0.1318, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13182570037753177, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 24% 980/4000 [00:27<01:20, 37.41it/s]Evaluating\n",
            "\n",
            "epoch:1000, time: 0.026769(s), valid (NDCG@10: 0.1232, HR@10: 0.1507), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12316962342928985, 0.1506849315068493) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 25% 1000/4000 [00:28<01:26, 34.84it/s]Evaluating\n",
            "\n",
            "epoch:1020, time: 0.027389(s), valid (NDCG@10: 0.1354, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13544782523589033, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 26% 1020/4000 [00:29<01:26, 34.52it/s]Evaluating\n",
            "\n",
            "epoch:1040, time: 0.027842(s), valid (NDCG@10: 0.1226, HR@10: 0.1507), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12260038638058962, 0.1506849315068493) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 26% 1040/4000 [00:29<01:21, 36.38it/s]Evaluating\n",
            "\n",
            "epoch:1060, time: 0.028308(s), valid (NDCG@10: 0.1233, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12330793425388949, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 26% 1060/4000 [00:30<01:16, 38.32it/s]Evaluating\n",
            "\n",
            "epoch:1080, time: 0.028752(s), valid (NDCG@10: 0.1323, HR@10: 0.1781), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.13225373945632427, 0.1780821917808219) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 27% 1080/4000 [00:30<01:13, 39.58it/s]Evaluating\n",
            "\n",
            "epoch:1100, time: 0.029218(s), valid (NDCG@10: 0.1359, HR@10: 0.1918), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13594939894827124, 0.1917808219178082) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 28% 1100/4000 [00:31<01:10, 41.01it/s]Evaluating\n",
            "\n",
            "epoch:1120, time: 0.029743(s), valid (NDCG@10: 0.1303, HR@10: 0.1644), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.13027519728590706, 0.1643835616438356) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 28% 1120/4000 [00:31<01:09, 41.62it/s]Evaluating\n",
            "\n",
            "epoch:1140, time: 0.030185(s), valid (NDCG@10: 0.1247, HR@10: 0.1507), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12474786563870989, 0.1506849315068493) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 28% 1140/4000 [00:31<01:07, 42.39it/s]Evaluating\n",
            "\n",
            "epoch:1160, time: 0.030642(s), valid (NDCG@10: 0.1220, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12199396847626845, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 29% 1160/4000 [00:32<01:06, 42.58it/s]Evaluating\n",
            "\n",
            "epoch:1180, time: 0.031072(s), valid (NDCG@10: 0.1243, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12431144671415156, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 30% 1180/4000 [00:32<01:05, 43.18it/s]Evaluating\n",
            "\n",
            "epoch:1200, time: 0.031507(s), valid (NDCG@10: 0.1228, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12284515975795407, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 30% 1200/4000 [00:33<01:04, 43.32it/s]Evaluating\n",
            "\n",
            "epoch:1220, time: 0.032222(s), valid (NDCG@10: 0.1269, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12693167747307257, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 30% 1220/4000 [00:33<01:03, 43.57it/s]Evaluating\n",
            "\n",
            "epoch:1240, time: 0.032662(s), valid (NDCG@10: 0.1135, HR@10: 0.1233), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.11354883323723132, 0.1232876712328767) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 31% 1240/4000 [00:34<01:03, 43.52it/s]Evaluating\n",
            "\n",
            "epoch:1260, time: 0.033126(s), valid (NDCG@10: 0.1239, HR@10: 0.1507), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12391435215821066, 0.1506849315068493) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 32% 1260/4000 [00:34<01:04, 42.67it/s]Evaluating\n",
            "\n",
            "epoch:1280, time: 0.033595(s), valid (NDCG@10: 0.1245, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12449196905267666, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 32% 1280/4000 [00:35<01:03, 43.05it/s]Evaluating\n",
            "\n",
            "epoch:1300, time: 0.034050(s), valid (NDCG@10: 0.1273, HR@10: 0.1644), test (NDCG@10: 0.0822, HR@10: 0.0822)\n",
            "(0.12731603045952017, 0.1643835616438356) (0.0821917808219178, 0.0821917808219178)\n",
            "\n",
            " 32% 1300/4000 [00:35<01:03, 42.71it/s]Evaluating\n",
            "\n",
            "epoch:1320, time: 0.034462(s), valid (NDCG@10: 0.1264, HR@10: 0.1644), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12639627209253204, 0.1643835616438356) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 33% 1320/4000 [00:36<01:02, 42.84it/s]Evaluating\n",
            "\n",
            "epoch:1340, time: 0.034916(s), valid (NDCG@10: 0.1275, HR@10: 0.1644), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12749106009758748, 0.1643835616438356) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 34% 1340/4000 [00:36<01:02, 42.53it/s]Evaluating\n",
            "\n",
            "epoch:1360, time: 0.035330(s), valid (NDCG@10: 0.1320, HR@10: 0.1781), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.13199784287029104, 0.1780821917808219) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 34% 1360/4000 [00:37<01:01, 43.02it/s]Evaluating\n",
            "\n",
            "epoch:1380, time: 0.035753(s), valid (NDCG@10: 0.1296, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12956925765856392, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 34% 1380/4000 [00:37<01:00, 43.05it/s]Evaluating\n",
            "\n",
            "epoch:1400, time: 0.036185(s), valid (NDCG@10: 0.1270, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12700268999685319, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 35% 1400/4000 [00:37<00:59, 43.55it/s]Evaluating\n",
            "\n",
            "epoch:1420, time: 0.036636(s), valid (NDCG@10: 0.1310, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.130997217555337, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 36% 1420/4000 [00:38<00:59, 43.71it/s]Evaluating\n",
            "\n",
            "epoch:1440, time: 0.037051(s), valid (NDCG@10: 0.1260, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12595376061760938, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 36% 1440/4000 [00:38<00:58, 43.71it/s]Evaluating\n",
            "\n",
            "epoch:1460, time: 0.037508(s), valid (NDCG@10: 0.1309, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1309286504700345, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 36% 1460/4000 [00:39<01:02, 40.54it/s]Evaluating\n",
            "\n",
            "epoch:1480, time: 0.038133(s), valid (NDCG@10: 0.1216, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12163232394931173, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 37% 1480/4000 [00:40<01:06, 38.14it/s]Evaluating\n",
            "\n",
            "epoch:1500, time: 0.038735(s), valid (NDCG@10: 0.1190, HR@10: 0.1370), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.11903480164988157, 0.136986301369863) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 38% 1500/4000 [00:40<01:10, 35.50it/s]Evaluating\n",
            "\n",
            "epoch:1520, time: 0.039390(s), valid (NDCG@10: 0.1307, HR@10: 0.1781), test (NDCG@10: 0.0822, HR@10: 0.0822)\n",
            "(0.13068387709267004, 0.1780821917808219) (0.0821917808219178, 0.0821917808219178)\n",
            "\n",
            " 38% 1520/4000 [00:41<01:14, 33.40it/s]Evaluating\n",
            "\n",
            "epoch:1540, time: 0.039834(s), valid (NDCG@10: 0.1270, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12695438593256342, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 38% 1540/4000 [00:41<01:08, 35.78it/s]Evaluating\n",
            "\n",
            "epoch:1560, time: 0.040303(s), valid (NDCG@10: 0.1265, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.1265263468537709, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 39% 1560/4000 [00:42<01:04, 37.82it/s]Evaluating\n",
            "\n",
            "epoch:1580, time: 0.040733(s), valid (NDCG@10: 0.1340, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1340372513296898, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 40% 1580/4000 [00:42<01:01, 39.25it/s]Evaluating\n",
            "\n",
            "epoch:1600, time: 0.041164(s), valid (NDCG@10: 0.1226, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12260038638058962, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 40% 1600/4000 [00:43<00:59, 40.64it/s]Evaluating\n",
            "\n",
            "epoch:1620, time: 0.041581(s), valid (NDCG@10: 0.1224, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12240264828303143, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 40% 1620/4000 [00:43<00:57, 41.20it/s]Evaluating\n",
            "\n",
            "epoch:1640, time: 0.042018(s), valid (NDCG@10: 0.1277, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12765496652698605, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 41% 1640/4000 [00:44<00:55, 42.24it/s]Evaluating\n",
            "\n",
            "epoch:1660, time: 0.042448(s), valid (NDCG@10: 0.1220, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12196013680810881, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 42% 1660/4000 [00:44<00:54, 42.71it/s]Evaluating\n",
            "\n",
            "epoch:1680, time: 0.042877(s), valid (NDCG@10: 0.1303, HR@10: 0.1781), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13032223256571332, 0.1780821917808219) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 42% 1680/4000 [00:45<00:54, 42.51it/s]Evaluating\n",
            "\n",
            "epoch:1700, time: 0.043341(s), valid (NDCG@10: 0.1322, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13219558096784922, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 42% 1700/4000 [00:45<00:54, 42.38it/s]Evaluating\n",
            "\n",
            "epoch:1720, time: 0.043768(s), valid (NDCG@10: 0.1354, HR@10: 0.1918), test (NDCG@10: 0.1918, HR@10: 0.1918)\n",
            "(0.1353524858917943, 0.1917808219178082) (0.1917808219178082, 0.1917808219178082)\n",
            "\n",
            " 43% 1720/4000 [00:45<00:53, 42.47it/s]Evaluating\n",
            "\n",
            "epoch:1740, time: 0.044216(s), valid (NDCG@10: 0.1142, HR@10: 0.1233), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.11415525114155249, 0.1232876712328767) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 44% 1740/4000 [00:46<00:53, 42.25it/s]Evaluating\n",
            "\n",
            "epoch:1760, time: 0.044669(s), valid (NDCG@10: 0.1234, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12335623831817923, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 44% 1760/4000 [00:46<00:52, 42.66it/s]Evaluating\n",
            "\n",
            "epoch:1780, time: 0.045110(s), valid (NDCG@10: 0.1320, HR@10: 0.1781), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13202343847508996, 0.1780821917808219) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 44% 1780/4000 [00:47<00:51, 42.93it/s]Evaluating\n",
            "\n",
            "epoch:1800, time: 0.045568(s), valid (NDCG@10: 0.1237, HR@10: 0.1507), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12372773726932125, 0.1506849315068493) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 45% 1800/4000 [00:47<00:51, 42.79it/s]Evaluating\n",
            "\n",
            "epoch:1820, time: 0.045991(s), valid (NDCG@10: 0.1323, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13233677893775692, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 46% 1820/4000 [00:48<00:50, 43.13it/s]Evaluating\n",
            "\n",
            "epoch:1840, time: 0.046475(s), valid (NDCG@10: 0.1315, HR@10: 0.1781), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.13149788751873467, 0.1780821917808219) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 46% 1840/4000 [00:48<00:49, 43.29it/s]Evaluating\n",
            "\n",
            "epoch:1860, time: 0.046877(s), valid (NDCG@10: 0.1232, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12315850022062104, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 46% 1860/4000 [00:49<00:49, 43.39it/s]Evaluating\n",
            "\n",
            "epoch:1880, time: 0.047345(s), valid (NDCG@10: 0.1240, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12401472216061113, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 47% 1880/4000 [00:49<00:49, 42.74it/s]Evaluating\n",
            "\n",
            "epoch:1900, time: 0.047782(s), valid (NDCG@10: 0.1285, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12850006525830734, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 48% 1900/4000 [00:50<00:48, 43.27it/s]Evaluating\n",
            "\n",
            "epoch:1920, time: 0.048230(s), valid (NDCG@10: 0.1184, HR@10: 0.1370), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1184283837455604, 0.136986301369863) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 48% 1920/4000 [00:50<00:47, 43.62it/s]Evaluating\n",
            "\n",
            "epoch:1940, time: 0.048685(s), valid (NDCG@10: 0.1267, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12667704967152285, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 48% 1940/4000 [00:51<00:47, 43.70it/s]Evaluating\n",
            "\n",
            "epoch:1960, time: 0.049109(s), valid (NDCG@10: 0.1322, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13222117657264815, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 49% 1960/4000 [00:51<00:49, 41.39it/s]Evaluating\n",
            "\n",
            "epoch:1980, time: 0.049748(s), valid (NDCG@10: 0.1350, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13495700969667793, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 50% 1980/4000 [00:52<00:52, 38.33it/s]Evaluating\n",
            "\n",
            "epoch:2000, time: 0.050354(s), valid (NDCG@10: 0.1322, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13219558096784922, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 50% 2000/4000 [00:52<00:55, 36.23it/s]Evaluating\n",
            "\n",
            "epoch:2020, time: 0.050985(s), valid (NDCG@10: 0.1339, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1339488528153366, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 50% 2020/4000 [00:53<00:58, 33.87it/s]Evaluating\n",
            "\n",
            "epoch:2040, time: 0.051637(s), valid (NDCG@10: 0.1215, HR@10: 0.1507), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.1214684175199132, 0.1506849315068493) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 51% 2040/4000 [00:54<00:56, 34.99it/s]Evaluating\n",
            "\n",
            "epoch:2060, time: 0.052027(s), valid (NDCG@10: 0.1180, HR@10: 0.1370), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.11803417633492753, 0.136986301369863) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 52% 2060/4000 [00:54<00:52, 37.14it/s]Evaluating\n",
            "\n",
            "epoch:2080, time: 0.052480(s), valid (NDCG@10: 0.1264, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.126396272092532, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 52% 2080/4000 [00:54<00:49, 38.85it/s]Evaluating\n",
            "\n",
            "epoch:2100, time: 0.052964(s), valid (NDCG@10: 0.1316, HR@10: 0.1781), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.1316374671278178, 0.1780821917808219) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 52% 2100/4000 [00:55<00:47, 39.62it/s]Evaluating\n",
            "\n",
            "epoch:2120, time: 0.053633(s), valid (NDCG@10: 0.1280, HR@10: 0.1644), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12804917393761892, 0.1643835616438356) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 53% 2120/4000 [00:56<01:00, 31.15it/s]Evaluating\n",
            "\n",
            "epoch:2140, time: 0.054331(s), valid (NDCG@10: 0.1247, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12474786563870989, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 54% 2140/4000 [00:57<01:10, 26.32it/s]Evaluating\n",
            "\n",
            "epoch:2160, time: 0.055565(s), valid (NDCG@10: 0.1309, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1309286504700345, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 54% 2160/4000 [00:58<01:10, 25.94it/s]Evaluating\n",
            "\n",
            "epoch:2180, time: 0.056229(s), valid (NDCG@10: 0.1398, HR@10: 0.2055), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.13981629718399424, 0.2054794520547945) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 55% 2180/4000 [00:58<01:08, 26.75it/s]Evaluating\n",
            "\n",
            "epoch:2200, time: 0.056832(s), valid (NDCG@10: 0.1271, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12707125708215572, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 55% 2200/4000 [00:59<01:15, 23.81it/s]Evaluating\n",
            "\n",
            "epoch:2220, time: 0.057482(s), valid (NDCG@10: 0.1353, HR@10: 0.1918), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13533095408629803, 0.1917808219178082) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 56% 2220/4000 [01:01<01:21, 21.79it/s]Evaluating\n",
            "\n",
            "epoch:2240, time: 0.057914(s), valid (NDCG@10: 0.1188, HR@10: 0.1370), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.11879002827251714, 0.136986301369863) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 56% 2240/4000 [01:01<01:10, 24.91it/s]Evaluating\n",
            "\n",
            "epoch:2260, time: 0.058533(s), valid (NDCG@10: 0.1304, HR@10: 0.1781), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13040527204714594, 0.1780821917808219) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 56% 2260/4000 [01:02<01:00, 28.53it/s]Evaluating\n",
            "\n",
            "epoch:2280, time: 0.058966(s), valid (NDCG@10: 0.1406, HR@10: 0.2055), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.14064071620688634, 0.2054794520547945) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 57% 2280/4000 [01:02<00:54, 31.54it/s]Evaluating\n",
            "\n",
            "epoch:2300, time: 0.059412(s), valid (NDCG@10: 0.1359, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13593492655214112, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 57% 2300/4000 [01:03<00:49, 34.63it/s]Evaluating\n",
            "\n",
            "epoch:2320, time: 0.059851(s), valid (NDCG@10: 0.1354, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13544782523589033, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 58% 2320/4000 [01:03<00:45, 37.00it/s]Evaluating\n",
            "\n",
            "epoch:2340, time: 0.060324(s), valid (NDCG@10: 0.1264, HR@10: 0.1644), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.1263624404243724, 0.1643835616438356) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 58% 2340/4000 [01:04<00:45, 36.27it/s]Evaluating\n",
            "\n",
            "epoch:2360, time: 0.060963(s), valid (NDCG@10: 0.1310, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.130997217555337, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 59% 2360/4000 [01:04<00:46, 35.19it/s]Evaluating\n",
            "\n",
            "epoch:2380, time: 0.061599(s), valid (NDCG@10: 0.1309, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13088161519022823, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 60% 2380/4000 [01:05<00:47, 33.98it/s]Evaluating\n",
            "\n",
            "epoch:2400, time: 0.062207(s), valid (NDCG@10: 0.1279, HR@10: 0.1644), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12785143584006076, 0.1643835616438356) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 60% 2400/4000 [01:06<01:06, 24.08it/s]Evaluating\n",
            "\n",
            "epoch:2420, time: 0.062893(s), valid (NDCG@10: 0.1312, HR@10: 0.1781), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13124325971718492, 0.1780821917808219) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 60% 2420/4000 [01:07<01:03, 24.75it/s]Evaluating\n",
            "\n",
            "epoch:2440, time: 0.063580(s), valid (NDCG@10: 0.1287, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12869780335586556, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 61% 2440/4000 [01:08<01:00, 25.90it/s]Evaluating\n",
            "\n",
            "epoch:2460, time: 0.064210(s), valid (NDCG@10: 0.1324, HR@10: 0.1781), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.13241764588572283, 0.1780821917808219) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 62% 2460/4000 [01:09<01:07, 22.76it/s]Evaluating\n",
            "\n",
            "epoch:2480, time: 0.064851(s), valid (NDCG@10: 0.1277, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1277020018067923, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 62% 2480/4000 [01:10<01:05, 23.18it/s]Evaluating\n",
            "\n",
            "epoch:2500, time: 0.065295(s), valid (NDCG@10: 0.1230, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12299459379122249, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 62% 2500/4000 [01:10<00:55, 27.04it/s]Evaluating\n",
            "\n",
            "epoch:2520, time: 0.065715(s), valid (NDCG@10: 0.1283, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12829394731498336, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 63% 2520/4000 [01:10<00:48, 30.53it/s]Evaluating\n",
            "\n",
            "epoch:2540, time: 0.066167(s), valid (NDCG@10: 0.1238, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12376994878324671, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 64% 2540/4000 [01:11<00:43, 33.65it/s]Evaluating\n",
            "\n",
            "epoch:2560, time: 0.066597(s), valid (NDCG@10: 0.1257, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12566762400569803, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 64% 2560/4000 [01:11<00:39, 36.22it/s]Evaluating\n",
            "\n",
            "epoch:2580, time: 0.067041(s), valid (NDCG@10: 0.1285, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12850006525830734, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 64% 2580/4000 [01:12<00:36, 38.42it/s]Evaluating\n",
            "\n",
            "epoch:2600, time: 0.067474(s), valid (NDCG@10: 0.1284, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1284108184645756, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 65% 2600/4000 [01:12<00:35, 39.98it/s]Evaluating\n",
            "\n",
            "epoch:2620, time: 0.067935(s), valid (NDCG@10: 0.1135, HR@10: 0.1233), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.11354883323723132, 0.1232876712328767) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 66% 2620/4000 [01:13<00:33, 41.11it/s]Evaluating\n",
            "\n",
            "epoch:2640, time: 0.068364(s), valid (NDCG@10: 0.1247, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12474786563870989, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 66% 2640/4000 [01:13<00:32, 41.73it/s]Evaluating\n",
            "\n",
            "epoch:2660, time: 0.068944(s), valid (NDCG@10: 0.1334, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13343777425511147, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 66% 2660/4000 [01:14<00:31, 42.38it/s]Evaluating\n",
            "\n",
            "epoch:2680, time: 0.069377(s), valid (NDCG@10: 0.1137, HR@10: 0.1233), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.11371273966662988, 0.1232876712328767) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 67% 2680/4000 [01:14<00:30, 42.85it/s]Evaluating\n",
            "\n",
            "epoch:2700, time: 0.069822(s), valid (NDCG@10: 0.1296, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12962741614703896, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 68% 2700/4000 [01:15<00:30, 42.82it/s]Evaluating\n",
            "\n",
            "epoch:2720, time: 0.070243(s), valid (NDCG@10: 0.1366, HR@10: 0.1918), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13656908357425762, 0.1917808219178082) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 68% 2720/4000 [01:15<00:29, 43.21it/s]Evaluating\n",
            "\n",
            "epoch:2740, time: 0.070711(s), valid (NDCG@10: 0.1315, HR@10: 0.1781), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.13149788751873467, 0.1780821917808219) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 68% 2740/4000 [01:16<00:29, 42.64it/s]Evaluating\n",
            "\n",
            "epoch:2760, time: 0.071128(s), valid (NDCG@10: 0.1402, HR@10: 0.2055), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.14022380033676263, 0.2054794520547945) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 69% 2760/4000 [01:16<00:29, 42.26it/s]Evaluating\n",
            "\n",
            "epoch:2780, time: 0.071727(s), valid (NDCG@10: 0.1359, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13592507212795582, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 70% 2780/4000 [01:17<00:32, 37.81it/s]Evaluating\n",
            "\n",
            "epoch:2800, time: 0.072357(s), valid (NDCG@10: 0.1330, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13301797123967968, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 70% 2800/4000 [01:18<00:43, 27.32it/s]Evaluating\n",
            "\n",
            "epoch:2820, time: 0.072966(s), valid (NDCG@10: 0.1264, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12639627209253204, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 70% 2820/4000 [01:19<00:42, 27.84it/s]Evaluating\n",
            "\n",
            "epoch:2840, time: 0.073615(s), valid (NDCG@10: 0.1372, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13715279301908795, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 71% 2840/4000 [01:19<00:38, 30.12it/s]Evaluating\n",
            "\n",
            "epoch:2860, time: 0.074011(s), valid (NDCG@10: 0.1332, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13316740527294813, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 72% 2860/4000 [01:20<00:34, 33.14it/s]Evaluating\n",
            "\n",
            "epoch:2880, time: 0.074486(s), valid (NDCG@10: 0.1318, HR@10: 0.1781), test (NDCG@10: 0.1781, HR@10: 0.1781)\n",
            "(0.13180137355721636, 0.1780821917808219) (0.1780821917808219, 0.1780821917808219)\n",
            "\n",
            " 72% 2880/4000 [01:20<00:31, 35.77it/s]Evaluating\n",
            "\n",
            "epoch:2900, time: 0.074915(s), valid (NDCG@10: 0.1346, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1345953651697212, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 72% 2900/4000 [01:20<00:29, 37.83it/s]Evaluating\n",
            "\n",
            "epoch:2920, time: 0.075320(s), valid (NDCG@10: 0.1260, HR@10: 0.1644), test (NDCG@10: 0.1918, HR@10: 0.1918)\n",
            "(0.12595376061760938, 0.1643835616438356) (0.1917808219178082, 0.1917808219178082)\n",
            "\n",
            " 73% 2920/4000 [01:21<00:27, 39.74it/s]Evaluating\n",
            "\n",
            "epoch:2940, time: 0.075769(s), valid (NDCG@10: 0.1196, HR@10: 0.1370), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.1196124185443476, 0.136986301369863) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 74% 2940/4000 [01:21<00:25, 40.89it/s]Evaluating\n",
            "\n",
            "epoch:2960, time: 0.076187(s), valid (NDCG@10: 0.1268, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12675791661948874, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 74% 2960/4000 [01:22<00:29, 35.30it/s]Evaluating\n",
            "\n",
            "epoch:2980, time: 0.076852(s), valid (NDCG@10: 0.1265, HR@10: 0.1644), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.12651187445764078, 0.1643835616438356) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 74% 2980/4000 [01:23<00:30, 32.95it/s]Evaluating\n",
            "\n",
            "epoch:3000, time: 0.077550(s), valid (NDCG@10: 0.1196, HR@10: 0.1370), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.1196124185443476, 0.136986301369863) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 75% 3000/4000 [01:24<00:34, 28.72it/s]Evaluating\n",
            "\n",
            "epoch:3020, time: 0.078233(s), valid (NDCG@10: 0.1232, HR@10: 0.1507), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12316962342928985, 0.1506849315068493) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 76% 3020/4000 [01:24<00:35, 27.93it/s]Evaluating\n",
            "\n",
            "epoch:3040, time: 0.078919(s), valid (NDCG@10: 0.1096, HR@10: 0.1096), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.1095890410958904, 0.1095890410958904) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 76% 3040/4000 [01:25<00:36, 26.26it/s]Evaluating\n",
            "\n",
            "epoch:3060, time: 0.079597(s), valid (NDCG@10: 0.1352, HR@10: 0.1918), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1352017830740424, 0.1917808219178082) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 76% 3060/4000 [01:26<00:37, 24.80it/s]Evaluating\n",
            "\n",
            "epoch:3080, time: 0.080370(s), valid (NDCG@10: 0.1230, HR@10: 0.1507), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12299459379122249, 0.1506849315068493) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 77% 3080/4000 [01:27<00:38, 23.77it/s]Evaluating\n",
            "\n",
            "epoch:3100, time: 0.081024(s), valid (NDCG@10: 0.1316, HR@10: 0.1781), test (NDCG@10: 0.1781, HR@10: 0.1781)\n",
            "(0.13160363545965817, 0.1780821917808219) (0.1780821917808219, 0.1780821917808219)\n",
            "\n",
            " 78% 3100/4000 [01:28<00:36, 24.97it/s]Evaluating\n",
            "\n",
            "epoch:3120, time: 0.081702(s), valid (NDCG@10: 0.1281, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12813842073135062, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 78% 3120/4000 [01:29<00:35, 24.48it/s]Evaluating\n",
            "\n",
            "epoch:3140, time: 0.082768(s), valid (NDCG@10: 0.1222, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1222387418536329, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 78% 3140/4000 [01:30<00:39, 21.61it/s]Evaluating\n",
            "\n",
            "epoch:3160, time: 0.083741(s), valid (NDCG@10: 0.1218, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12183006204686991, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 79% 3160/4000 [01:31<00:45, 18.33it/s]Evaluating\n",
            "\n",
            "epoch:3180, time: 0.084257(s), valid (NDCG@10: 0.1267, HR@10: 0.1644), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12672408495132909, 0.1643835616438356) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 80% 3180/4000 [01:32<00:41, 19.85it/s]Evaluating\n",
            "\n",
            "epoch:3200, time: 0.084673(s), valid (NDCG@10: 0.1369, HR@10: 0.1918), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.1369141121920878, 0.1917808219178082) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 80% 3200/4000 [01:33<00:33, 23.76it/s]Evaluating\n",
            "\n",
            "epoch:3220, time: 0.085110(s), valid (NDCG@10: 0.1301, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13007745918834887, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 80% 3220/4000 [01:33<00:28, 27.48it/s]Evaluating\n",
            "\n",
            "epoch:3240, time: 0.085534(s), valid (NDCG@10: 0.1230, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12297188533173166, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 81% 3240/4000 [01:34<00:24, 30.88it/s]Evaluating\n",
            "\n",
            "epoch:3260, time: 0.085965(s), valid (NDCG@10: 0.1281, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.1281384207313506, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 82% 3260/4000 [01:34<00:21, 33.76it/s]Evaluating\n",
            "\n",
            "epoch:3280, time: 0.086402(s), valid (NDCG@10: 0.1224, HR@10: 0.1507), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12243647995119109, 0.1506849315068493) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 82% 3280/4000 [01:35<00:22, 31.37it/s]Evaluating\n",
            "\n",
            "epoch:3300, time: 0.087085(s), valid (NDCG@10: 0.1314, HR@10: 0.1781), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.13143972903025963, 0.1780821917808219) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 82% 3300/4000 [01:36<00:23, 29.49it/s]Evaluating\n",
            "\n",
            "epoch:3320, time: 0.087523(s), valid (NDCG@10: 0.1386, HR@10: 0.2055), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13855515731106213, 0.2054794520547945) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 83% 3320/4000 [01:36<00:20, 32.70it/s]Evaluating\n",
            "\n",
            "epoch:3340, time: 0.088003(s), valid (NDCG@10: 0.1312, HR@10: 0.1781), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.13120725551555856, 0.1780821917808219) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 84% 3340/4000 [01:36<00:18, 35.24it/s]Evaluating\n",
            "\n",
            "epoch:3360, time: 0.088665(s), valid (NDCG@10: 0.1285, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.1284578537443819, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 84% 3360/4000 [01:37<00:17, 37.05it/s]Evaluating\n",
            "\n",
            "epoch:3380, time: 0.089692(s), valid (NDCG@10: 0.1260, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12595376061760938, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 84% 3380/4000 [01:37<00:16, 38.44it/s]Evaluating\n",
            "\n",
            "epoch:3400, time: 0.090097(s), valid (NDCG@10: 0.1275, HR@10: 0.1644), test (NDCG@10: 0.1781, HR@10: 0.1781)\n",
            "(0.12753200282702942, 0.1643835616438356) (0.1780821917808219, 0.1780821917808219)\n",
            "\n",
            " 85% 3400/4000 [01:38<00:15, 39.18it/s]Evaluating\n",
            "\n",
            "epoch:3420, time: 0.090538(s), valid (NDCG@10: 0.1267, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12670961255519897, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 86% 3420/4000 [01:38<00:14, 39.49it/s]Evaluating\n",
            "\n",
            "epoch:3440, time: 0.090978(s), valid (NDCG@10: 0.1272, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12723516351155428, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 86% 3440/4000 [01:39<00:13, 40.79it/s]Evaluating\n",
            "\n",
            "epoch:3460, time: 0.091416(s), valid (NDCG@10: 0.1241, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12408938179627799, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 86% 3460/4000 [01:39<00:12, 41.65it/s]Evaluating\n",
            "\n",
            "epoch:3480, time: 0.091823(s), valid (NDCG@10: 0.1177, HR@10: 0.1370), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.1176725318079708, 0.136986301369863) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 87% 3480/4000 [01:40<00:12, 42.52it/s]Evaluating\n",
            "\n",
            "epoch:3500, time: 0.092304(s), valid (NDCG@10: 0.1411, HR@10: 0.2055), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.14114138617028404, 0.2054794520547945) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 88% 3500/4000 [01:40<00:11, 42.88it/s]Evaluating\n",
            "\n",
            "epoch:3520, time: 0.092743(s), valid (NDCG@10: 0.1182, HR@10: 0.1370), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.11823191443248572, 0.136986301369863) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 88% 3520/4000 [01:41<00:11, 43.49it/s]Evaluating\n",
            "\n",
            "epoch:3540, time: 0.093187(s), valid (NDCG@10: 0.1288, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12884328354476163, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 88% 3540/4000 [01:41<00:10, 43.29it/s]Evaluating\n",
            "\n",
            "epoch:3560, time: 0.093644(s), valid (NDCG@10: 0.1273, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12729332200002932, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 89% 3560/4000 [01:42<00:10, 43.62it/s]Evaluating\n",
            "\n",
            "epoch:3580, time: 0.094076(s), valid (NDCG@10: 0.1260, HR@10: 0.1644), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.1260346275655753, 0.1643835616438356) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 90% 3580/4000 [01:42<00:10, 41.83it/s]Evaluating\n",
            "\n",
            "epoch:3600, time: 0.094694(s), valid (NDCG@10: 0.1316, HR@10: 0.1781), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.13156890004251529, 0.1780821917808219) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 90% 3600/4000 [01:43<00:10, 38.25it/s]Evaluating\n",
            "\n",
            "epoch:3620, time: 0.095279(s), valid (NDCG@10: 0.1283, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1282712388554925, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 90% 3620/4000 [01:43<00:10, 36.00it/s]Evaluating\n",
            "\n",
            "epoch:3640, time: 0.095962(s), valid (NDCG@10: 0.1278, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.1277932773515857, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 91% 3640/4000 [01:44<00:10, 33.60it/s]Evaluating\n",
            "\n",
            "epoch:3660, time: 0.096641(s), valid (NDCG@10: 0.1344, HR@10: 0.1918), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.13436506418848687, 0.1917808219178082) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 92% 3660/4000 [01:45<00:10, 33.60it/s]Evaluating\n",
            "\n",
            "epoch:3680, time: 0.097112(s), valid (NDCG@10: 0.1322, HR@10: 0.1781), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.1321617492996896, 0.1780821917808219) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 92% 3680/4000 [01:45<00:08, 35.98it/s]Evaluating\n",
            "\n",
            "epoch:3700, time: 0.097541(s), valid (NDCG@10: 0.1277, HR@10: 0.1644), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.12772471026628315, 0.1643835616438356) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 92% 3700/4000 [01:46<00:07, 37.52it/s]Evaluating\n",
            "\n",
            "epoch:3720, time: 0.097964(s), valid (NDCG@10: 0.1281, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12809138545154436, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 93% 3720/4000 [01:46<00:07, 38.97it/s]Evaluating\n",
            "\n",
            "epoch:3740, time: 0.098419(s), valid (NDCG@10: 0.1279, HR@10: 0.1644), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12787414429955157, 0.1643835616438356) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 94% 3740/4000 [01:47<00:06, 39.72it/s]Evaluating\n",
            "\n",
            "epoch:3760, time: 0.098845(s), valid (NDCG@10: 0.1188, HR@10: 0.1370), test (NDCG@10: 0.1644, HR@10: 0.1644)\n",
            "(0.11879002827251714, 0.136986301369863) (0.1643835616438356, 0.1643835616438356)\n",
            "\n",
            " 94% 3760/4000 [01:47<00:05, 40.20it/s]Evaluating\n",
            "\n",
            "epoch:3780, time: 0.099279(s), valid (NDCG@10: 0.1262, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12615149871516756, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 94% 3780/4000 [01:48<00:05, 40.66it/s]Evaluating\n",
            "\n",
            "epoch:3800, time: 0.099717(s), valid (NDCG@10: 0.1230, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12299459379122249, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 95% 3800/4000 [01:48<00:04, 40.87it/s]Evaluating\n",
            "\n",
            "epoch:3820, time: 0.100145(s), valid (NDCG@10: 0.1294, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12944970144908277, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            " 96% 3820/4000 [01:48<00:04, 41.76it/s]Evaluating\n",
            "\n",
            "epoch:3840, time: 0.100584(s), valid (NDCG@10: 0.1227, HR@10: 0.1507), test (NDCG@10: 0.0685, HR@10: 0.0685)\n",
            "(0.12271598874569843, 0.1506849315068493) (0.0684931506849315, 0.0684931506849315)\n",
            "\n",
            " 96% 3840/4000 [01:49<00:03, 42.00it/s]Evaluating\n",
            "\n",
            "epoch:3860, time: 0.101064(s), valid (NDCG@10: 0.1234, HR@10: 0.1507), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12335623831817923, 0.1506849315068493) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            " 96% 3860/4000 [01:49<00:03, 41.31it/s]Evaluating\n",
            "\n",
            "epoch:3880, time: 0.101751(s), valid (NDCG@10: 0.1390, HR@10: 0.1918), test (NDCG@10: 0.1096, HR@10: 0.1096)\n",
            "(0.13899659197205264, 0.1917808219178082) (0.1095890410958904, 0.1095890410958904)\n",
            "\n",
            " 97% 3880/4000 [01:50<00:02, 41.66it/s]Evaluating\n",
            "\n",
            "epoch:3900, time: 0.102192(s), valid (NDCG@10: 0.1183, HR@10: 0.1370), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.11827894971229197, 0.136986301369863) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 98% 3900/4000 [01:50<00:02, 42.08it/s]Evaluating\n",
            "\n",
            "epoch:3920, time: 0.102647(s), valid (NDCG@10: 0.1285, HR@10: 0.1644), test (NDCG@10: 0.1233, HR@10: 0.1233)\n",
            "(0.12850006525830737, 0.1643835616438356) (0.1232876712328767, 0.1232876712328767)\n",
            "\n",
            " 98% 3920/4000 [01:51<00:01, 42.35it/s]Evaluating\n",
            "\n",
            "epoch:3940, time: 0.103122(s), valid (NDCG@10: 0.1218, HR@10: 0.1507), test (NDCG@10: 0.0822, HR@10: 0.0822)\n",
            "(0.12179623037871026, 0.1506849315068493) (0.0821917808219178, 0.0821917808219178)\n",
            "\n",
            " 98% 3940/4000 [01:51<00:01, 42.96it/s]Evaluating\n",
            "\n",
            "epoch:3960, time: 0.103541(s), valid (NDCG@10: 0.1226, HR@10: 0.1507), test (NDCG@10: 0.1507, HR@10: 0.1507)\n",
            "(0.12255208231629987, 0.1506849315068493) (0.1506849315068493, 0.1506849315068493)\n",
            "\n",
            " 99% 3960/4000 [01:52<00:00, 42.08it/s]Evaluating\n",
            "\n",
            "epoch:3980, time: 0.103986(s), valid (NDCG@10: 0.1267, HR@10: 0.1644), test (NDCG@10: 0.1370, HR@10: 0.1370)\n",
            "(0.12667578088703935, 0.1643835616438356) (0.136986301369863, 0.136986301369863)\n",
            "\n",
            "100% 3980/4000 [01:52<00:00, 42.60it/s]Evaluating\n",
            "\n",
            "epoch:4000, time: 0.104423(s), valid (NDCG@10: 0.1233, HR@10: 0.1507), test (NDCG@10: 0.0959, HR@10: 0.0959)\n",
            "(0.12330793425388949, 0.1506849315068493) (0.0958904109589041, 0.0958904109589041)\n",
            "\n",
            "100% 4000/4000 [01:53<00:00, 35.24it/s]\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "!python main.py --device=cuda --dataset Movies_and_TV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au4aeUnZSepQ",
        "outputId": "2cd74d0a-5eaf-4718-f0d5-d3e0a0b0e1a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/allmrec_CrossModal/A-LLMRec\n"
          ]
        }
      ],
      "source": [
        "%cd ../../\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip Movies_and_TV.json\n",
        "!mv Movies_and_TV.json.gz meta_Movies_and_TV.json allmrec_CrossModal/A-LLMRec/data/amazon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2jhp601PdiN",
        "outputId": "3168a247-abb5-40b0-a5e3-0ca73214765f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: Movies_and_TV.json: No such file or directory\n",
            "mv: cannot stat 'Movies_and_TV.json.gz': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6o60m5AVjVi",
        "outputId": "97272175-8678-4f59-d173-9c7c5ae3abf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-19 21:19:37.282187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-19 21:19:37.316011: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-19 21:19:37.327386: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-19 21:19:37.351387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-19 21:19:38.893565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "A-LLMRec start train phase-1\n",
            "\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/recsys_model.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  kwargs, checkpoint = torch.load(pth_file_path[0], map_location=\"cpu\")\n",
            "modules.json: 100% 229/229 [00:00<00:00, 1.46MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 831kB/s]\n",
            "README.md: 100% 3.73k/3.73k [00:00<00:00, 24.9MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 391kB/s]\n",
            "config.json: 100% 540/540 [00:00<00:00, 3.46MB/s]\n",
            "model.safetensors: 100% 265M/265M [00:01<00:00, 236MB/s]\n",
            "tokenizer_config.json: 100% 554/554 [00:00<00:00, 3.48MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 10.8MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 5.11MB/s]\n",
            "special_tokens_map.json: 100% 112/112 [00:00<00:00, 733kB/s]\n",
            "1_Pooling/config.json: 100% 190/190 [00:00<00:00, 754kB/s]\n",
            "user num: 73 item num: 67\n",
            "average sequence length: 4.47\n",
            "Initializing with num_user: 73\n",
            "  0% 0/10 [00:00<?, ?it/s]loss in epoch 1/11 iteration 0/2: 1.6259698867797852 / BPR loss: 1.3873438835144043 / Matching loss: 0.008426226675510406 / Item reconstruction: 0.2564288079738617 / Text reconstruction: 0.5099272727966309\n",
            "loss in epoch 1/11 iteration 1/2: 1.613808274269104 / BPR loss: 1.3869563341140747 / Matching loss: 0.00713396305218339 / Item reconstruction: 0.24926984310150146 / Text reconstruction: 0.47541582584381104\n",
            "loss in epoch 1/11 iteration 2/2: 1.5974832773208618 / BPR loss: 1.386879563331604 / Matching loss: 0.005194677039980888 / Item reconstruction: 0.26128992438316345 / Text reconstruction: 0.3738204836845398\n",
            " 10% 1/10 [00:06<01:00,  6.70s/it]loss in epoch 2/11 iteration 0/2: 1.5847430229187012 / BPR loss: 1.3868275880813599 / Matching loss: 0.005005130544304848 / Item reconstruction: 0.23884759843349457 / Text reconstruction: 0.36743271350860596\n",
            "loss in epoch 2/11 iteration 1/2: 1.579079508781433 / BPR loss: 1.3865766525268555 / Matching loss: 0.00435219518840313 / Item reconstruction: 0.23342612385749817 / Text reconstruction: 0.3571878671646118\n",
            "loss in epoch 2/11 iteration 2/2: 1.568735957145691 / BPR loss: 1.386425495147705 / Matching loss: 0.0036251882556825876 / Item reconstruction: 0.2381003499031067 / Text reconstruction: 0.29817575216293335\n",
            " 20% 2/10 [00:13<00:55,  6.92s/it]loss in epoch 3/11 iteration 0/2: 1.5597513914108276 / BPR loss: 1.3872408866882324 / Matching loss: 0.0035268431529402733 / Item reconstruction: 0.22030912339687347 / Text reconstruction: 0.29414618015289307\n",
            "loss in epoch 3/11 iteration 1/2: 1.5562841892242432 / BPR loss: 1.3865933418273926 / Matching loss: 0.003330409061163664 / Item reconstruction: 0.21240603923797607 / Text reconstruction: 0.30078792572021484\n",
            "loss in epoch 3/11 iteration 2/2: 1.5425268411636353 / BPR loss: 1.3860753774642944 / Matching loss: 0.002481606788933277 / Item reconstruction: 0.21374838054180145 / Text reconstruction: 0.23547805845737457\n",
            " 30% 3/10 [00:21<00:51,  7.37s/it]loss in epoch 4/11 iteration 0/2: 1.5445291996002197 / BPR loss: 1.3866139650344849 / Matching loss: 0.0027433400973677635 / Item reconstruction: 0.20567521452903748 / Text reconstruction: 0.2616717517375946\n",
            "loss in epoch 4/11 iteration 1/2: 1.5411285161972046 / BPR loss: 1.3866567611694336 / Matching loss: 0.002669356297701597 / Item reconstruction: 0.2017994225025177 / Text reconstruction: 0.254513680934906\n",
            "loss in epoch 4/11 iteration 2/2: 1.535889983177185 / BPR loss: 1.3868091106414795 / Matching loss: 0.002290265401825309 / Item reconstruction: 0.20214629173278809 / Text reconstruction: 0.228587344288826\n",
            " 40% 4/10 [00:26<00:37,  6.33s/it]loss in epoch 5/11 iteration 0/2: 1.5260530710220337 / BPR loss: 1.3867703676223755 / Matching loss: 0.0023386848624795675 / Item reconstruction: 0.18425261974334717 / Text reconstruction: 0.2240891456604004\n",
            "loss in epoch 5/11 iteration 1/2: 1.5240086317062378 / BPR loss: 1.3866204023361206 / Matching loss: 0.002227491233497858 / Item reconstruction: 0.181876540184021 / Text reconstruction: 0.2211121916770935\n",
            "loss in epoch 5/11 iteration 2/2: 1.5240668058395386 / BPR loss: 1.3874256610870361 / Matching loss: 0.002116539515554905 / Item reconstruction: 0.18355557322502136 / Text reconstruction: 0.21373388171195984\n",
            " 50% 5/10 [00:34<00:34,  6.83s/it]loss in epoch 6/11 iteration 0/2: 1.517040729522705 / BPR loss: 1.3871110677719116 / Matching loss: 0.0020949463360011578 / Item reconstruction: 0.17415456473827362 / Text reconstruction: 0.20378684997558594\n",
            "loss in epoch 6/11 iteration 1/2: 1.5138967037200928 / BPR loss: 1.3870577812194824 / Matching loss: 0.0019905969966202974 / Item reconstruction: 0.1693294495344162 / Text reconstruction: 0.2009185254573822\n",
            "loss in epoch 6/11 iteration 2/2: 1.513427972793579 / BPR loss: 1.3866488933563232 / Matching loss: 0.0018070356454700232 / Item reconstruction: 0.17322248220443726 / Text reconstruction: 0.19180339574813843\n",
            " 60% 6/10 [00:39<00:25,  6.25s/it]loss in epoch 7/11 iteration 0/2: 1.504809021949768 / BPR loss: 1.386785626411438 / Matching loss: 0.0017797488253563643 / Item reconstruction: 0.1591006964445114 / Text reconstruction: 0.18346580862998962\n",
            "loss in epoch 7/11 iteration 1/2: 1.5046005249023438 / BPR loss: 1.3869463205337524 / Matching loss: 0.0016954084858298302 / Item reconstruction: 0.15886035561561584 / Text reconstruction: 0.18264269828796387\n",
            "loss in epoch 7/11 iteration 2/2: 1.5057119131088257 / BPR loss: 1.3869049549102783 / Matching loss: 0.0016310522332787514 / Item reconstruction: 0.16529971361160278 / Text reconstruction: 0.17263032495975494\n",
            " 70% 7/10 [00:46<00:19,  6.50s/it]loss in epoch 8/11 iteration 0/2: 1.497013807296753 / BPR loss: 1.3867757320404053 / Matching loss: 0.0015311534516513348 / Item reconstruction: 0.14851392805576324 / Text reconstruction: 0.17224979400634766\n",
            "loss in epoch 8/11 iteration 1/2: 1.493618130683899 / BPR loss: 1.3868794441223145 / Matching loss: 0.0014239647425711155 / Item reconstruction: 0.14403136074543 / Text reconstruction: 0.16649535298347473\n",
            "loss in epoch 8/11 iteration 2/2: 1.494262933731079 / BPR loss: 1.3864282369613647 / Matching loss: 0.001295907306484878 / Item reconstruction: 0.14893795549869537 / Text reconstruction: 0.16034871339797974\n",
            " 80% 8/10 [00:51<00:12,  6.04s/it]loss in epoch 9/11 iteration 0/2: 1.4872912168502808 / BPR loss: 1.386901617050171 / Matching loss: 0.0013156504137441516 / Item reconstruction: 0.13572466373443604 / Text reconstruction: 0.15605849027633667\n",
            "loss in epoch 9/11 iteration 1/2: 1.485885500907898 / BPR loss: 1.3866946697235107 / Matching loss: 0.0012814521323889494 / Item reconstruction: 0.13480094075202942 / Text reconstruction: 0.15254443883895874\n",
            "loss in epoch 9/11 iteration 2/2: 1.4856642484664917 / BPR loss: 1.3865635395050049 / Matching loss: 0.0011699151946231723 / Item reconstruction: 0.13681527972221375 / Text reconstruction: 0.14761558175086975\n",
            " 90% 9/10 [01:00<00:06,  6.93s/it]loss in epoch 10/11 iteration 0/2: 1.4796777963638306 / BPR loss: 1.3865045309066772 / Matching loss: 0.0011931221233680844 / Item reconstruction: 0.12651140987873077 / Text reconstruction: 0.1436222493648529\n",
            "loss in epoch 10/11 iteration 1/2: 1.477832555770874 / BPR loss: 1.3862643241882324 / Matching loss: 0.0011546968016773462 / Item reconstruction: 0.1255994290113449 / Text reconstruction: 0.1380693018436432\n",
            "loss in epoch 10/11 iteration 2/2: 1.4780571460723877 / BPR loss: 1.3861533403396606 / Matching loss: 0.0011204313486814499 / Item reconstruction: 0.12912672758102417 / Text reconstruction: 0.13110007345676422\n",
            "100% 10/10 [01:05<00:00,  6.51s/it]\n",
            "train time : 65.1318724155426\n"
          ]
        }
      ],
      "source": [
        "!python main.py --pretrain_stage1 --rec_pre_trained_data Movies_and_TV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfeE0BcCvvxp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWau2O53WBCj",
        "outputId": "bbc6159b-d203-41cc-dca6-252e004036d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-19 21:20:58.165897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-19 21:20:58.187546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-19 21:20:58.197375: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-19 21:20:58.216422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-19 21:20:59.400545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "A-LLMRec strat train phase-2\n",
            "\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/recsys_model.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  kwargs, checkpoint = torch.load(pth_file_path[0], map_location=\"cpu\")\n",
            "config.json: 100% 651/651 [00:00<00:00, 3.37MB/s]\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "pytorch_model.bin.index.json: 100% 41.9k/41.9k [00:00<00:00, 655kB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.96G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 21.0M/9.96G [00:00<00:58, 168MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 52.4M/9.96G [00:00<00:45, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.96G [00:00<00:42, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 115M/9.96G [00:00<00:42, 231MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.96G [00:00<00:41, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 178M/9.96G [00:00<00:44, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.96G [00:00<00:43, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 241M/9.96G [00:01<00:41, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.96G [00:01<00:41, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 304M/9.96G [00:01<00:41, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.96G [00:01<00:41, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 367M/9.96G [00:01<00:41, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/9.96G [00:01<00:43, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 430M/9.96G [00:01<00:44, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/9.96G [00:02<00:43, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 493M/9.96G [00:02<00:45, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 514M/9.96G [00:02<00:47, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 535M/9.96G [00:02<00:46, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 566M/9.96G [00:02<01:00, 156MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.96G [00:02<00:57, 163MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 608M/9.96G [00:02<00:54, 170MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 640M/9.96G [00:03<00:48, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 671M/9.96G [00:03<00:43, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 703M/9.96G [00:03<00:40, 226MB/s]\u001b[A\n",
            "\n",
            "model.safetensors.index.json: 100% 44.0k/44.0k [00:00<00:00, 92.1MB/s]\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   7% 734M/9.96G [00:03<00:46, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 765M/9.96G [00:03<00:46, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 786M/9.96G [00:03<00:47, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 807M/9.96G [00:03<00:47, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 828M/9.96G [00:04<00:46, 195MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 849M/9.96G [00:04<00:45, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 881M/9.96G [00:04<00:44, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 912M/9.96G [00:04<00:42, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 944M/9.96G [00:04<00:40, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 975M/9.96G [00:04<00:40, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.01G/9.96G [00:04<00:38, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.04G/9.96G [00:04<00:36, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.07G/9.96G [00:05<00:37, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.10G/9.96G [00:05<00:39, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.13G/9.96G [00:05<00:38, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.16G/9.96G [00:05<00:37, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.20G/9.96G [00:05<00:35, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.23G/9.96G [00:05<00:36, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.26G/9.96G [00:05<00:35, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.29G/9.96G [00:05<00:34, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.32G/9.96G [00:06<00:34, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.35G/9.96G [00:06<00:33, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.38G/9.96G [00:06<00:34, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.42G/9.96G [00:06<00:34, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.45G/9.96G [00:06<00:35, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.48G/9.96G [00:06<00:34, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.51G/9.96G [00:06<00:33, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.54G/9.96G [00:06<00:34, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.57G/9.96G [00:07<00:33, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.60G/9.96G [00:07<00:34, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.64G/9.96G [00:07<00:33, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.67G/9.96G [00:07<00:32, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.70G/9.96G [00:07<00:32, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.73G/9.96G [00:07<00:33, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.76G/9.96G [00:07<00:32, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.79G/9.96G [00:07<00:32, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.82G/9.96G [00:08<00:32, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.86G/9.96G [00:08<00:32, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.89G/9.96G [00:08<00:39, 202MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.92G/9.96G [00:08<00:37, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.95G/9.96G [00:08<00:36, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.98G/9.96G [00:08<00:35, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.01G/9.96G [00:08<00:34, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.04G/9.96G [00:09<00:33, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.08G/9.96G [00:09<00:33, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.11G/9.96G [00:09<00:42, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.14G/9.96G [00:09<00:38, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.17G/9.96G [00:09<00:36, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.20G/9.96G [00:09<00:35, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.23G/9.96G [00:10<00:35, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.26G/9.96G [00:10<00:33, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.30G/9.96G [00:10<00:32, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.33G/9.96G [00:15<06:33, 19.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.36G/9.96G [00:15<04:43, 26.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.39G/9.96G [00:15<03:26, 36.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.42G/9.96G [00:15<02:35, 48.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.44G/9.96G [00:15<02:10, 57.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.46G/9.96G [00:16<01:48, 68.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.49G/9.96G [00:16<01:29, 83.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.52G/9.96G [00:16<01:09, 106MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.55G/9.96G [00:16<00:55, 133MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.58G/9.96G [00:16<00:46, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.61G/9.96G [00:16<00:43, 170MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.64G/9.96G [00:16<00:38, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.67G/9.96G [00:16<00:37, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.71G/9.96G [00:17<00:37, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.74G/9.96G [00:17<00:38, 187MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.76G/9.96G [00:17<00:39, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.78G/9.96G [00:17<00:40, 178MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.80G/9.96G [00:21<06:31, 18.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.83G/9.96G [00:21<04:19, 27.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.86G/9.96G [00:21<03:00, 39.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.89G/9.96G [00:21<02:09, 54.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.93G/9.96G [00:22<01:37, 71.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.96G/9.96G [00:22<01:16, 91.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.99G/9.96G [00:22<01:00, 115MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.02G/9.96G [00:22<00:49, 140MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.05G/9.96G [00:22<00:43, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.08G/9.96G [00:22<00:38, 180MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.11G/9.96G [00:22<00:33, 202MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.15G/9.96G [00:22<00:32, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.18G/9.96G [00:23<00:30, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.21G/9.96G [00:23<00:28, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.24G/9.96G [00:23<00:29, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.27G/9.96G [00:23<00:27, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.30G/9.96G [00:23<00:27, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.33G/9.96G [00:23<00:26, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.37G/9.96G [00:23<00:27, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.40G/9.96G [00:24<00:31, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.43G/9.96G [00:24<00:40, 163MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.45G/9.96G [00:27<04:14, 25.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.48G/9.96G [00:27<02:58, 36.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.51G/9.96G [00:27<02:09, 49.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.54G/9.96G [00:27<01:38, 65.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.96G [00:28<01:24, 75.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.59G/9.96G [00:28<01:12, 88.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.62G/9.96G [00:28<00:56, 112MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.65G/9.96G [00:28<00:46, 136MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.68G/9.96G [00:28<00:40, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.71G/9.96G [00:28<00:35, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.74G/9.96G [00:28<00:34, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.77G/9.96G [00:29<00:30, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.81G/9.96G [00:29<00:29, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.84G/9.96G [00:29<00:31, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.87G/9.96G [00:29<00:32, 186MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.89G/9.96G [00:29<00:37, 164MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.91G/9.96G [00:33<05:22, 18.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.94G/9.96G [00:34<03:38, 27.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.97G/9.96G [00:34<02:33, 39.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.01G/9.96G [00:34<01:52, 53.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.04G/9.96G [00:34<01:24, 70.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.07G/9.96G [00:34<01:05, 89.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.10G/9.96G [00:34<00:52, 112MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.13G/9.96G [00:34<00:43, 134MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.16G/9.96G [00:34<00:36, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.19G/9.96G [00:35<00:33, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.23G/9.96G [00:35<00:30, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.26G/9.96G [00:35<00:27, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.29G/9.96G [00:35<00:26, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.32G/9.96G [00:35<00:24, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.35G/9.96G [00:35<00:24, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.38G/9.96G [00:35<00:23, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.41G/9.96G [00:35<00:22, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.45G/9.96G [00:36<00:22, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.48G/9.96G [00:36<00:22, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.51G/9.96G [00:36<00:22, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.54G/9.96G [00:36<00:22, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.57G/9.96G [00:36<00:22, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.60G/9.96G [00:36<00:22, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.63G/9.96G [00:36<00:22, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.67G/9.96G [00:36<00:21, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.70G/9.96G [00:37<00:21, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.73G/9.96G [00:37<00:22, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.76G/9.96G [00:37<00:21, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.79G/9.96G [00:37<00:20, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.82G/9.96G [00:37<00:20, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.85G/9.96G [00:37<00:21, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.89G/9.96G [00:37<00:21, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.92G/9.96G [00:38<00:25, 196MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.95G/9.96G [00:38<00:24, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.98G/9.96G [00:38<00:22, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.01G/9.96G [00:38<00:23, 212MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.04G/9.96G [00:38<00:23, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.08G/9.96G [00:38<00:21, 226MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.11G/9.96G [00:38<00:20, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.14G/9.96G [00:39<00:19, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.17G/9.96G [00:39<00:19, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.20G/9.96G [00:39<00:19, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.23G/9.96G [00:39<00:19, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.26G/9.96G [00:39<00:18, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.30G/9.96G [00:39<00:18, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.33G/9.96G [00:39<00:18, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.36G/9.96G [00:39<00:18, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.39G/9.96G [00:40<00:18, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.42G/9.96G [00:40<00:17, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.45G/9.96G [00:40<00:17, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.48G/9.96G [00:40<00:17, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.52G/9.96G [00:40<00:18, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.55G/9.96G [00:40<00:18, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.58G/9.96G [00:40<00:18, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.61G/9.96G [00:40<00:18, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.64G/9.96G [00:41<00:18, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.67G/9.96G [00:41<00:21, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.69G/9.96G [00:41<00:21, 200MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.73G/9.96G [00:41<00:20, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.76G/9.96G [00:43<01:55, 36.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.79G/9.96G [00:44<01:23, 50.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.82G/9.96G [00:44<01:02, 66.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.85G/9.96G [00:44<00:48, 84.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.88G/9.96G [00:44<00:39, 104MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.96G [00:44<00:32, 125MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.95G/9.96G [00:44<00:26, 149MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.96G [00:44<00:23, 171MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.01G/9.96G [00:44<00:20, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.04G/9.96G [00:45<00:19, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.07G/9.96G [00:45<00:18, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.10G/9.96G [00:45<00:17, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.13G/9.96G [00:45<00:16, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.17G/9.96G [00:45<00:16, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.20G/9.96G [00:45<00:16, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.23G/9.96G [00:45<00:15, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.26G/9.96G [00:46<00:15, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.29G/9.96G [00:46<00:16, 226MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.32G/9.96G [00:46<00:15, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.35G/9.96G [00:46<00:15, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.39G/9.96G [00:46<00:14, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.42G/9.96G [00:46<00:14, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.45G/9.96G [00:46<00:13, 258MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.48G/9.96G [00:46<00:13, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.51G/9.96G [00:47<00:13, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.54G/9.96G [00:47<00:13, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.57G/9.96G [00:47<00:13, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.61G/9.96G [00:47<00:14, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.64G/9.96G [00:47<00:13, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.67G/9.96G [00:47<00:13, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.70G/9.96G [00:47<00:13, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.73G/9.96G [00:47<00:12, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.76G/9.96G [00:48<00:12, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.79G/9.96G [00:48<00:12, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.83G/9.96G [00:48<00:12, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.86G/9.96G [00:48<00:12, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.89G/9.96G [00:48<00:12, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.92G/9.96G [00:48<00:12, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.95G/9.96G [00:48<00:12, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.98G/9.96G [00:48<00:12, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.01G/9.96G [00:49<00:11, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.05G/9.96G [00:49<00:11, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.08G/9.96G [00:49<00:11, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.11G/9.96G [00:49<00:11, 258MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.14G/9.96G [00:49<00:11, 255MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.17G/9.96G [00:49<00:11, 250MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.20G/9.96G [00:49<00:11, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.24G/9.96G [00:49<00:11, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.27G/9.96G [00:50<00:10, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.30G/9.96G [00:50<00:10, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.33G/9.96G [00:50<00:10, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.36G/9.96G [00:50<00:10, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.39G/9.96G [00:50<00:10, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.42G/9.96G [00:50<00:10, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.46G/9.96G [00:50<00:11, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.49G/9.96G [00:51<00:10, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.52G/9.96G [00:51<00:09, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.55G/9.96G [00:51<00:10, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.58G/9.96G [00:51<00:10, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.61G/9.96G [00:51<00:09, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.64G/9.96G [00:51<00:09, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.68G/9.96G [00:51<00:09, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.71G/9.96G [00:51<00:08, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.74G/9.96G [00:52<00:09, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.77G/9.96G [00:52<00:09, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.80G/9.96G [00:52<00:08, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.83G/9.96G [00:52<00:08, 245MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.86G/9.96G [00:52<00:08, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.90G/9.96G [00:53<00:18, 111MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.92G/9.96G [00:53<00:16, 122MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.95G/9.96G [00:53<00:13, 148MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.98G/9.96G [00:53<00:11, 172MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.01G/9.96G [00:53<00:10, 179MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.04G/9.96G [00:53<00:10, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.07G/9.96G [00:54<00:09, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.11G/9.96G [00:54<00:09, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.14G/9.96G [00:54<00:08, 218MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.17G/9.96G [00:54<00:08, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.20G/9.96G [00:54<00:08, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.23G/9.96G [00:54<00:07, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.26G/9.96G [00:54<00:07, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.29G/9.96G [00:54<00:07, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.33G/9.96G [00:55<00:08, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.36G/9.96G [00:55<00:07, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.39G/9.96G [00:55<00:08, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.41G/9.96G [01:00<01:28, 17.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.44G/9.96G [01:00<01:00, 25.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.47G/9.96G [01:00<00:42, 35.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.50G/9.96G [01:00<00:30, 48.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.54G/9.96G [01:01<00:22, 62.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.57G/9.96G [01:01<00:17, 79.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.60G/9.96G [01:01<00:13, 101MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.63G/9.96G [01:01<00:10, 123MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.66G/9.96G [01:01<00:08, 145MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.69G/9.96G [01:01<00:07, 164MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.72G/9.96G [01:01<00:07, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.76G/9.96G [01:01<00:06, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.79G/9.96G [01:02<00:05, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.82G/9.96G [01:06<00:52, 22.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.85G/9.96G [01:06<00:36, 30.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.88G/9.96G [01:06<00:26, 41.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.91G/9.96G [01:06<00:19, 54.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.94G/9.96G [01:07<00:15, 67.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.97G/9.96G [01:07<00:12, 77.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.00G/9.96G [01:07<00:09, 98.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.03G/9.96G [01:07<00:07, 118MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.06G/9.96G [01:07<00:06, 141MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.09G/9.96G [01:07<00:05, 157MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.12G/9.96G [01:08<00:05, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.14G/9.96G [01:08<00:04, 166MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.18G/9.96G [01:08<00:04, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.21G/9.96G [01:08<00:03, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.24G/9.96G [01:08<00:03, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.27G/9.96G [01:08<00:03, 210MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.30G/9.96G [01:08<00:03, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.33G/9.96G [01:08<00:02, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.36G/9.96G [01:09<00:03, 179MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.38G/9.96G [01:09<00:03, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.42G/9.96G [01:09<00:02, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.45G/9.96G [01:09<00:02, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.48G/9.96G [01:12<00:16, 29.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.52G/9.96G [01:12<00:09, 44.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.55G/9.96G [01:12<00:07, 58.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.58G/9.96G [01:13<00:05, 74.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.62G/9.96G [01:13<00:03, 91.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.65G/9.96G [01:13<00:02, 113MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.68G/9.96G [01:13<00:02, 134MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.71G/9.96G [01:13<00:01, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.74G/9.96G [01:13<00:01, 175MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.77G/9.96G [01:13<00:00, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.80G/9.96G [01:14<00:00, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.84G/9.96G [01:14<00:00, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.87G/9.96G [01:14<00:00, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.90G/9.96G [01:18<00:02, 21.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.93G/9.96G [01:18<00:01, 30.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.96G/9.96G [01:18<00:00, 126MB/s] \n",
            "Downloading shards:  50% 1/2 [01:19<01:19, 79.17s/it]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.36G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 21.0M/3.36G [00:00<00:19, 175MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 41.9M/3.36G [00:00<00:18, 179MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 62.9M/3.36G [00:00<00:19, 173MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 83.9M/3.36G [00:00<00:19, 164MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 105M/3.36G [00:00<00:19, 170MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 126M/3.36G [00:00<00:18, 173MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 157M/3.36G [00:00<00:16, 199MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 189M/3.36G [00:00<00:15, 207MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 210M/3.36G [00:01<00:15, 199MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 231M/3.36G [00:01<00:16, 187MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 262M/3.36G [00:01<00:15, 194MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 294M/3.36G [00:01<00:15, 200MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 315M/3.36G [00:03<01:39, 30.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 346M/3.36G [00:04<01:08, 44.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 377M/3.36G [00:04<00:48, 61.4MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 409M/3.36G [00:04<00:36, 80.6MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 440M/3.36G [00:04<00:28, 101MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 472M/3.36G [00:04<00:23, 122MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 503M/3.36G [00:04<00:19, 144MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 535M/3.36G [00:04<00:16, 168MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 566M/3.36G [00:05<00:14, 187MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 598M/3.36G [00:05<00:13, 201MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 629M/3.36G [00:05<00:12, 211MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 661M/3.36G [00:05<00:12, 223MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 692M/3.36G [00:05<00:11, 231MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 724M/3.36G [00:05<00:11, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 755M/3.36G [00:05<00:11, 230MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 786M/3.36G [00:05<00:10, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 818M/3.36G [00:06<00:10, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 849M/3.36G [00:06<00:10, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 881M/3.36G [00:06<00:10, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 912M/3.36G [00:06<00:10, 235MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 944M/3.36G [00:06<00:10, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 975M/3.36G [00:06<00:09, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.01G/3.36G [00:06<00:09, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.04G/3.36G [00:06<00:09, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.07G/3.36G [00:07<00:09, 236MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.10G/3.36G [00:07<00:09, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.13G/3.36G [00:07<00:09, 234MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.16G/3.36G [00:07<00:09, 234MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.20G/3.36G [00:07<00:09, 235MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.23G/3.36G [00:07<00:09, 224MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.26G/3.36G [00:07<00:09, 225MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.29G/3.36G [00:08<00:09, 223MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.32G/3.36G [00:08<00:08, 236MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.35G/3.36G [00:08<00:08, 235MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.38G/3.36G [00:08<00:08, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.42G/3.36G [00:08<00:08, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.45G/3.36G [00:08<00:07, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.48G/3.36G [00:08<00:07, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.51G/3.36G [00:08<00:07, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.54G/3.36G [00:09<00:07, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.57G/3.36G [00:09<00:07, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.60G/3.36G [00:09<00:07, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.64G/3.36G [00:09<00:07, 233MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.67G/3.36G [00:09<00:07, 239MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.70G/3.36G [00:10<00:11, 146MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.72G/3.36G [00:13<01:17, 21.1MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 1.75G/3.36G [00:14<00:53, 30.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 1.77G/3.36G [00:14<00:45, 34.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 1.80G/3.36G [00:14<00:31, 49.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 1.84G/3.36G [00:14<00:22, 67.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 1.87G/3.36G [00:14<00:17, 85.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.90G/3.36G [00:14<00:13, 108MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 1.93G/3.36G [00:15<00:10, 131MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 1.96G/3.36G [00:15<00:09, 154MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 1.99G/3.36G [00:15<00:07, 173MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.02G/3.36G [00:15<00:07, 182MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.06G/3.36G [00:15<00:06, 199MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.09G/3.36G [00:15<00:05, 215MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.12G/3.36G [00:15<00:05, 224MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.15G/3.36G [00:15<00:05, 221MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.18G/3.36G [00:16<00:10, 111MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.20G/3.36G [00:20<00:51, 22.2MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.23G/3.36G [00:20<00:35, 31.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.26G/3.36G [00:20<00:24, 44.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.30G/3.36G [00:20<00:17, 59.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.33G/3.36G [00:20<00:13, 76.0MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.36G/3.36G [00:20<00:10, 93.5MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.39G/3.36G [00:20<00:08, 114MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.42G/3.36G [00:21<00:06, 138MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.45G/3.36G [00:21<00:05, 159MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.49G/3.36G [00:21<00:04, 184MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.52G/3.36G [00:21<00:04, 199MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.55G/3.36G [00:21<00:03, 206MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.58G/3.36G [00:21<00:03, 221MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 2.61G/3.36G [00:21<00:03, 224MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 2.64G/3.36G [00:21<00:03, 223MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 2.67G/3.36G [00:22<00:02, 236MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 2.71G/3.36G [00:22<00:02, 239MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.74G/3.36G [00:22<00:02, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 2.77G/3.36G [00:22<00:02, 228MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 2.80G/3.36G [00:22<00:02, 229MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 2.83G/3.36G [00:22<00:02, 225MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 2.86G/3.36G [00:22<00:02, 229MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 2.89G/3.36G [00:22<00:01, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 2.93G/3.36G [00:23<00:01, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 2.96G/3.36G [00:23<00:01, 241MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 2.99G/3.36G [00:23<00:01, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.02G/3.36G [00:23<00:01, 247MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.05G/3.36G [00:23<00:01, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.08G/3.36G [00:23<00:01, 241MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.11G/3.36G [00:23<00:00, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.15G/3.36G [00:24<00:00, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.18G/3.36G [00:24<00:01, 124MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.20G/3.36G [00:24<00:01, 105MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.23G/3.36G [00:24<00:00, 130MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.25G/3.36G [00:25<00:00, 116MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.27G/3.36G [00:25<00:00, 125MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.29G/3.36G [00:25<00:00, 118MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.31G/3.36G [00:25<00:00, 132MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.36G/3.36G [00:25<00:00, 130MB/s]\n",
            "Downloading shards: 100% 2/2 [01:45<00:00, 52.58s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [01:03<00:00, 31.61s/it]\n",
            "generation_config.json: 100% 137/137 [00:00<00:00, 687kB/s]\n",
            "tokenizer_config.json: 100% 685/685 [00:00<00:00, 4.13MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 4.61MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 2.47MB/s]\n",
            "special_tokens_map.json: 100% 441/441 [00:00<00:00, 2.65MB/s]\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/a_llmrec_model.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mlp = torch.load(out_dir + 'mlp.pt', map_location = args.device)\n",
            "user num: 73 item num: 67\n",
            "average sequence length: 4.47\n",
            "Initializing with num_user: 73\n",
            "  0% 0/10 [00:00<?, ?it/s]/content/allmrec_CrossModal/A-LLMRec/models/llm4rec.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "A-LLMRec model loss in epoch 1/11 iteration 0/36: 7.380208492279053\n",
            "A-LLMRec model loss in epoch 1/11 iteration 1/36: 5.7008056640625\n",
            "A-LLMRec model loss in epoch 1/11 iteration 2/36: 5.425911903381348\n",
            "A-LLMRec model loss in epoch 1/11 iteration 3/36: 3.8065240383148193\n",
            "A-LLMRec model loss in epoch 1/11 iteration 4/36: 2.7575511932373047\n",
            "A-LLMRec model loss in epoch 1/11 iteration 5/36: 1.111694097518921\n",
            "A-LLMRec model loss in epoch 1/11 iteration 6/36: 3.6797072887420654\n",
            "A-LLMRec model loss in epoch 1/11 iteration 7/36: 3.3107874393463135\n",
            "A-LLMRec model loss in epoch 1/11 iteration 8/36: 2.214966297149658\n",
            "A-LLMRec model loss in epoch 1/11 iteration 9/36: 2.881187677383423\n",
            "A-LLMRec model loss in epoch 1/11 iteration 10/36: 2.2224843502044678\n",
            "A-LLMRec model loss in epoch 1/11 iteration 11/36: 1.9965555667877197\n",
            "A-LLMRec model loss in epoch 1/11 iteration 12/36: 1.9549201726913452\n",
            "A-LLMRec model loss in epoch 1/11 iteration 13/36: 1.5733132362365723\n",
            "A-LLMRec model loss in epoch 1/11 iteration 14/36: 1.6197007894515991\n",
            "A-LLMRec model loss in epoch 1/11 iteration 15/36: 2.9742271900177\n",
            "A-LLMRec model loss in epoch 1/11 iteration 16/36: 1.7108001708984375\n",
            "A-LLMRec model loss in epoch 1/11 iteration 17/36: 1.194117546081543\n",
            "A-LLMRec model loss in epoch 1/11 iteration 18/36: 1.717765212059021\n",
            "A-LLMRec model loss in epoch 1/11 iteration 19/36: 1.1767076253890991\n",
            "A-LLMRec model loss in epoch 1/11 iteration 20/36: 0.8813156485557556\n",
            "A-LLMRec model loss in epoch 1/11 iteration 21/36: 0.9038865566253662\n",
            "A-LLMRec model loss in epoch 1/11 iteration 22/36: 1.0080326795578003\n",
            "A-LLMRec model loss in epoch 1/11 iteration 23/36: 0.7504205703735352\n",
            "A-LLMRec model loss in epoch 1/11 iteration 24/36: 0.6945493817329407\n",
            "A-LLMRec model loss in epoch 1/11 iteration 25/36: 0.48498785495758057\n",
            "A-LLMRec model loss in epoch 1/11 iteration 26/36: 0.47533538937568665\n",
            "A-LLMRec model loss in epoch 1/11 iteration 27/36: 0.818566620349884\n",
            "A-LLMRec model loss in epoch 1/11 iteration 28/36: 0.7983003258705139\n",
            "A-LLMRec model loss in epoch 1/11 iteration 29/36: 0.3332873284816742\n",
            "A-LLMRec model loss in epoch 1/11 iteration 30/36: 0.49420973658561707\n",
            "A-LLMRec model loss in epoch 1/11 iteration 31/36: 0.7616937160491943\n",
            "A-LLMRec model loss in epoch 1/11 iteration 32/36: 0.6285990476608276\n",
            "A-LLMRec model loss in epoch 1/11 iteration 33/36: 0.5546168684959412\n",
            "A-LLMRec model loss in epoch 1/11 iteration 34/36: 0.6223077178001404\n",
            "A-LLMRec model loss in epoch 1/11 iteration 35/36: 0.31177255511283875\n",
            "A-LLMRec model loss in epoch 1/11 iteration 36/36: 0.3401798903942108\n",
            " 10% 1/10 [01:52<16:49, 112.17s/it]A-LLMRec model loss in epoch 2/11 iteration 0/36: 0.10806503891944885\n",
            "A-LLMRec model loss in epoch 2/11 iteration 1/36: 0.657048761844635\n",
            "A-LLMRec model loss in epoch 2/11 iteration 2/36: 0.451164573431015\n",
            "A-LLMRec model loss in epoch 2/11 iteration 3/36: 0.6764405965805054\n",
            "A-LLMRec model loss in epoch 2/11 iteration 4/36: 0.562995433807373\n",
            "A-LLMRec model loss in epoch 2/11 iteration 5/36: 0.2759377360343933\n",
            "A-LLMRec model loss in epoch 2/11 iteration 6/36: 0.3870420455932617\n",
            "A-LLMRec model loss in epoch 2/11 iteration 7/36: 0.2750714421272278\n",
            "A-LLMRec model loss in epoch 2/11 iteration 8/36: 0.337369441986084\n",
            "A-LLMRec model loss in epoch 2/11 iteration 9/36: 0.2320653200149536\n",
            "A-LLMRec model loss in epoch 2/11 iteration 10/36: 0.4191930592060089\n",
            "A-LLMRec model loss in epoch 2/11 iteration 11/36: 0.39541250467300415\n",
            "A-LLMRec model loss in epoch 2/11 iteration 12/36: 0.22198231518268585\n",
            "A-LLMRec model loss in epoch 2/11 iteration 13/36: 0.19594860076904297\n",
            "A-LLMRec model loss in epoch 2/11 iteration 14/36: 0.4333637058734894\n",
            "A-LLMRec model loss in epoch 2/11 iteration 15/36: 0.49210721254348755\n",
            "A-LLMRec model loss in epoch 2/11 iteration 16/36: 0.7782527804374695\n",
            "A-LLMRec model loss in epoch 2/11 iteration 17/36: 0.32687169313430786\n",
            "A-LLMRec model loss in epoch 2/11 iteration 18/36: 0.3105146288871765\n",
            "A-LLMRec model loss in epoch 2/11 iteration 19/36: 0.1945538967847824\n",
            "A-LLMRec model loss in epoch 2/11 iteration 20/36: 0.1899247169494629\n",
            "A-LLMRec model loss in epoch 2/11 iteration 21/36: 0.21189023554325104\n",
            "A-LLMRec model loss in epoch 2/11 iteration 22/36: 0.3671671748161316\n",
            "A-LLMRec model loss in epoch 2/11 iteration 23/36: 0.315974622964859\n",
            "A-LLMRec model loss in epoch 2/11 iteration 24/36: 0.3751015067100525\n",
            "A-LLMRec model loss in epoch 2/11 iteration 25/36: 0.6450991034507751\n",
            "A-LLMRec model loss in epoch 2/11 iteration 26/36: 0.2676263153553009\n",
            "A-LLMRec model loss in epoch 2/11 iteration 27/36: 0.2684851288795471\n",
            "A-LLMRec model loss in epoch 2/11 iteration 28/36: 0.46293023228645325\n",
            "A-LLMRec model loss in epoch 2/11 iteration 29/36: 0.2654462158679962\n",
            "A-LLMRec model loss in epoch 2/11 iteration 30/36: 0.10903028398752213\n",
            "A-LLMRec model loss in epoch 2/11 iteration 31/36: 0.15858952701091766\n",
            "A-LLMRec model loss in epoch 2/11 iteration 32/36: 0.4566075801849365\n",
            "A-LLMRec model loss in epoch 2/11 iteration 33/36: 0.5643731951713562\n",
            "A-LLMRec model loss in epoch 2/11 iteration 34/36: 0.2661047875881195\n",
            "A-LLMRec model loss in epoch 2/11 iteration 35/36: 0.8289174437522888\n",
            "A-LLMRec model loss in epoch 2/11 iteration 36/36: 0.7586188316345215\n",
            " 20% 2/10 [03:46<15:07, 113.45s/it]A-LLMRec model loss in epoch 3/11 iteration 0/36: 0.4962593615055084\n",
            "A-LLMRec model loss in epoch 3/11 iteration 1/36: 0.26404017210006714\n",
            "A-LLMRec model loss in epoch 3/11 iteration 2/36: 0.03973070904612541\n",
            "A-LLMRec model loss in epoch 3/11 iteration 3/36: 0.41512244939804077\n",
            "A-LLMRec model loss in epoch 3/11 iteration 4/36: 0.46982061862945557\n",
            "A-LLMRec model loss in epoch 3/11 iteration 5/36: 0.43971776962280273\n",
            "A-LLMRec model loss in epoch 3/11 iteration 6/36: 0.22555598616600037\n",
            "A-LLMRec model loss in epoch 3/11 iteration 7/36: 0.7173407673835754\n",
            "A-LLMRec model loss in epoch 3/11 iteration 8/36: 0.05456021428108215\n",
            "A-LLMRec model loss in epoch 3/11 iteration 9/36: 0.6716698408126831\n",
            "A-LLMRec model loss in epoch 3/11 iteration 10/36: 0.3050996661186218\n",
            "A-LLMRec model loss in epoch 3/11 iteration 11/36: 0.12805302441120148\n",
            "A-LLMRec model loss in epoch 3/11 iteration 12/36: 0.20288841426372528\n",
            "A-LLMRec model loss in epoch 3/11 iteration 13/36: 0.7478979229927063\n",
            "A-LLMRec model loss in epoch 3/11 iteration 14/36: 0.3446894884109497\n",
            "A-LLMRec model loss in epoch 3/11 iteration 15/36: 0.263053834438324\n",
            "A-LLMRec model loss in epoch 3/11 iteration 16/36: 0.45499733090400696\n",
            "A-LLMRec model loss in epoch 3/11 iteration 17/36: 0.2817804217338562\n",
            "A-LLMRec model loss in epoch 3/11 iteration 18/36: 0.3269561231136322\n",
            "A-LLMRec model loss in epoch 3/11 iteration 19/36: 0.3844582736492157\n",
            "A-LLMRec model loss in epoch 3/11 iteration 20/36: 0.2763842046260834\n",
            "A-LLMRec model loss in epoch 3/11 iteration 21/36: 0.3558027148246765\n",
            "A-LLMRec model loss in epoch 3/11 iteration 22/36: 0.4202321171760559\n",
            "A-LLMRec model loss in epoch 3/11 iteration 23/36: 0.606964111328125\n",
            "A-LLMRec model loss in epoch 3/11 iteration 24/36: 0.6507453322410583\n",
            "A-LLMRec model loss in epoch 3/11 iteration 25/36: 0.2500738203525543\n",
            "A-LLMRec model loss in epoch 3/11 iteration 26/36: 0.03270397707819939\n",
            "A-LLMRec model loss in epoch 3/11 iteration 27/36: 0.3432382345199585\n",
            "A-LLMRec model loss in epoch 3/11 iteration 28/36: 0.5608183145523071\n",
            "A-LLMRec model loss in epoch 3/11 iteration 29/36: 0.3709970712661743\n",
            "A-LLMRec model loss in epoch 3/11 iteration 30/36: 0.2676324248313904\n",
            "A-LLMRec model loss in epoch 3/11 iteration 31/36: 0.3762094974517822\n",
            "A-LLMRec model loss in epoch 3/11 iteration 32/36: 0.5653041005134583\n",
            "A-LLMRec model loss in epoch 3/11 iteration 33/36: 0.7241883873939514\n",
            "A-LLMRec model loss in epoch 3/11 iteration 34/36: 0.3284708261489868\n",
            "A-LLMRec model loss in epoch 3/11 iteration 35/36: 0.19196006655693054\n",
            "A-LLMRec model loss in epoch 3/11 iteration 36/36: 0.26133644580841064\n",
            " 30% 3/10 [05:40<13:16, 113.73s/it]A-LLMRec model loss in epoch 4/11 iteration 0/36: 0.6116133332252502\n",
            "A-LLMRec model loss in epoch 4/11 iteration 1/36: 0.5434442758560181\n",
            "A-LLMRec model loss in epoch 4/11 iteration 2/36: 0.2672571837902069\n",
            "A-LLMRec model loss in epoch 4/11 iteration 3/36: 0.18647684156894684\n",
            "A-LLMRec model loss in epoch 4/11 iteration 4/36: 0.06663554161787033\n",
            "A-LLMRec model loss in epoch 4/11 iteration 5/36: 0.2665617763996124\n",
            "A-LLMRec model loss in epoch 4/11 iteration 6/36: 0.19786781072616577\n",
            "A-LLMRec model loss in epoch 4/11 iteration 7/36: 0.4013277292251587\n",
            "A-LLMRec model loss in epoch 4/11 iteration 8/36: 0.22112853825092316\n",
            "A-LLMRec model loss in epoch 4/11 iteration 9/36: 0.24186034500598907\n",
            "A-LLMRec model loss in epoch 4/11 iteration 10/36: 0.6837354898452759\n",
            "A-LLMRec model loss in epoch 4/11 iteration 11/36: 0.2958666980266571\n",
            "A-LLMRec model loss in epoch 4/11 iteration 12/36: 0.18964417278766632\n",
            "A-LLMRec model loss in epoch 4/11 iteration 13/36: 0.391988605260849\n",
            "A-LLMRec model loss in epoch 4/11 iteration 14/36: 0.29314693808555603\n",
            "A-LLMRec model loss in epoch 4/11 iteration 15/36: 0.15109576284885406\n",
            "A-LLMRec model loss in epoch 4/11 iteration 16/36: 0.32483068108558655\n",
            "A-LLMRec model loss in epoch 4/11 iteration 17/36: 0.44149208068847656\n",
            "A-LLMRec model loss in epoch 4/11 iteration 18/36: 0.326061874628067\n",
            "A-LLMRec model loss in epoch 4/11 iteration 19/36: 0.20551680028438568\n",
            "A-LLMRec model loss in epoch 4/11 iteration 20/36: 0.09634009748697281\n",
            "A-LLMRec model loss in epoch 4/11 iteration 21/36: 0.19517800211906433\n",
            "A-LLMRec model loss in epoch 4/11 iteration 22/36: 0.1874193698167801\n",
            "A-LLMRec model loss in epoch 4/11 iteration 23/36: 0.39417216181755066\n",
            "A-LLMRec model loss in epoch 4/11 iteration 24/36: 0.18489105999469757\n",
            "A-LLMRec model loss in epoch 4/11 iteration 25/36: 0.3282744288444519\n",
            "A-LLMRec model loss in epoch 4/11 iteration 26/36: 0.33622369170188904\n",
            "A-LLMRec model loss in epoch 4/11 iteration 27/36: 0.2521398067474365\n",
            "A-LLMRec model loss in epoch 4/11 iteration 28/36: 0.0823548436164856\n",
            "A-LLMRec model loss in epoch 4/11 iteration 29/36: 0.3676236867904663\n",
            "A-LLMRec model loss in epoch 4/11 iteration 30/36: 0.23576904833316803\n",
            "A-LLMRec model loss in epoch 4/11 iteration 31/36: 0.2966773211956024\n",
            "A-LLMRec model loss in epoch 4/11 iteration 32/36: 0.621107280254364\n",
            "A-LLMRec model loss in epoch 4/11 iteration 33/36: 0.24800850450992584\n",
            "A-LLMRec model loss in epoch 4/11 iteration 34/36: 0.354371041059494\n",
            "A-LLMRec model loss in epoch 4/11 iteration 35/36: 0.34261354804039\n",
            "A-LLMRec model loss in epoch 4/11 iteration 36/36: 0.0517798475921154\n",
            " 40% 4/10 [07:34<11:22, 113.71s/it]A-LLMRec model loss in epoch 5/11 iteration 0/36: 0.2593819797039032\n",
            "A-LLMRec model loss in epoch 5/11 iteration 1/36: 0.2533018887042999\n",
            "A-LLMRec model loss in epoch 5/11 iteration 2/36: 0.4551301598548889\n",
            "A-LLMRec model loss in epoch 5/11 iteration 3/36: 0.579358696937561\n",
            "A-LLMRec model loss in epoch 5/11 iteration 4/36: 0.3046065866947174\n",
            "A-LLMRec model loss in epoch 5/11 iteration 5/36: 0.27229058742523193\n",
            "A-LLMRec model loss in epoch 5/11 iteration 6/36: 0.6027812957763672\n",
            "A-LLMRec model loss in epoch 5/11 iteration 7/36: 0.5001998543739319\n",
            "A-LLMRec model loss in epoch 5/11 iteration 8/36: 0.26402029395103455\n",
            "A-LLMRec model loss in epoch 5/11 iteration 9/36: 0.2651657164096832\n",
            "A-LLMRec model loss in epoch 5/11 iteration 10/36: 0.06334381550550461\n",
            "A-LLMRec model loss in epoch 5/11 iteration 11/36: 0.8105141520500183\n",
            "A-LLMRec model loss in epoch 5/11 iteration 12/36: 0.17448173463344574\n",
            "A-LLMRec model loss in epoch 5/11 iteration 13/36: 0.15374596416950226\n",
            "A-LLMRec model loss in epoch 5/11 iteration 14/36: 0.5836320519447327\n",
            "A-LLMRec model loss in epoch 5/11 iteration 15/36: 0.2909245193004608\n",
            "A-LLMRec model loss in epoch 5/11 iteration 16/36: 0.30327677726745605\n",
            "A-LLMRec model loss in epoch 5/11 iteration 17/36: 0.5257391929626465\n",
            "A-LLMRec model loss in epoch 5/11 iteration 18/36: 0.03285953029990196\n",
            "A-LLMRec model loss in epoch 5/11 iteration 19/36: 0.3053777813911438\n",
            "A-LLMRec model loss in epoch 5/11 iteration 20/36: 0.10870470106601715\n",
            "A-LLMRec model loss in epoch 5/11 iteration 21/36: 0.1097627654671669\n",
            "A-LLMRec model loss in epoch 5/11 iteration 22/36: 0.52674400806427\n",
            "A-LLMRec model loss in epoch 5/11 iteration 23/36: 0.38416698575019836\n",
            "A-LLMRec model loss in epoch 5/11 iteration 24/36: 0.5168682336807251\n",
            "A-LLMRec model loss in epoch 5/11 iteration 25/36: 0.06081617623567581\n",
            "A-LLMRec model loss in epoch 5/11 iteration 26/36: 0.28577250242233276\n",
            "A-LLMRec model loss in epoch 5/11 iteration 27/36: 0.2731831669807434\n",
            "A-LLMRec model loss in epoch 5/11 iteration 28/36: 0.11167663335800171\n",
            "A-LLMRec model loss in epoch 5/11 iteration 29/36: 0.1782829910516739\n",
            "A-LLMRec model loss in epoch 5/11 iteration 30/36: 0.6038732528686523\n",
            "A-LLMRec model loss in epoch 5/11 iteration 31/36: 0.16491571068763733\n",
            "A-LLMRec model loss in epoch 5/11 iteration 32/36: 0.1641891896724701\n",
            "A-LLMRec model loss in epoch 5/11 iteration 33/36: 0.026579292491078377\n",
            "A-LLMRec model loss in epoch 5/11 iteration 34/36: 0.15781231224536896\n",
            "A-LLMRec model loss in epoch 5/11 iteration 35/36: 0.2771536707878113\n",
            "A-LLMRec model loss in epoch 5/11 iteration 36/36: 0.5384450554847717\n",
            " 50% 5/10 [09:28<09:29, 113.85s/it]A-LLMRec model loss in epoch 6/11 iteration 0/36: 0.6284019947052002\n",
            "A-LLMRec model loss in epoch 6/11 iteration 1/36: 0.47663530707359314\n",
            "A-LLMRec model loss in epoch 6/11 iteration 2/36: 0.2382621020078659\n",
            "A-LLMRec model loss in epoch 6/11 iteration 3/36: 0.35861262679100037\n",
            "A-LLMRec model loss in epoch 6/11 iteration 4/36: 0.11349156498908997\n",
            "A-LLMRec model loss in epoch 6/11 iteration 5/36: 0.3910268247127533\n",
            "A-LLMRec model loss in epoch 6/11 iteration 6/36: 0.15824314951896667\n",
            "A-LLMRec model loss in epoch 6/11 iteration 7/36: 0.12072519958019257\n",
            "A-LLMRec model loss in epoch 6/11 iteration 8/36: 0.4124402403831482\n",
            "A-LLMRec model loss in epoch 6/11 iteration 9/36: 0.2813917100429535\n",
            "A-LLMRec model loss in epoch 6/11 iteration 10/36: 0.5399000644683838\n",
            "A-LLMRec model loss in epoch 6/11 iteration 11/36: 0.1446356475353241\n",
            "A-LLMRec model loss in epoch 6/11 iteration 12/36: 0.27288201451301575\n",
            "A-LLMRec model loss in epoch 6/11 iteration 13/36: 0.252736359834671\n",
            "A-LLMRec model loss in epoch 6/11 iteration 14/36: 0.432720810174942\n",
            "A-LLMRec model loss in epoch 6/11 iteration 15/36: 0.35462620854377747\n",
            "A-LLMRec model loss in epoch 6/11 iteration 16/36: 0.3725772500038147\n",
            "A-LLMRec model loss in epoch 6/11 iteration 17/36: 0.3192186653614044\n",
            "A-LLMRec model loss in epoch 6/11 iteration 18/36: 0.658493161201477\n",
            "A-LLMRec model loss in epoch 6/11 iteration 19/36: 0.08000843971967697\n",
            "A-LLMRec model loss in epoch 6/11 iteration 20/36: 0.36346322298049927\n",
            "A-LLMRec model loss in epoch 6/11 iteration 21/36: 0.18772144615650177\n",
            "A-LLMRec model loss in epoch 6/11 iteration 22/36: 0.31032904982566833\n",
            "A-LLMRec model loss in epoch 6/11 iteration 23/36: 0.27644869685173035\n",
            "A-LLMRec model loss in epoch 6/11 iteration 24/36: 0.04097851365804672\n",
            "A-LLMRec model loss in epoch 6/11 iteration 25/36: 0.3395777940750122\n",
            "A-LLMRec model loss in epoch 6/11 iteration 26/36: 0.40374964475631714\n",
            "A-LLMRec model loss in epoch 6/11 iteration 27/36: 0.09588849544525146\n",
            "A-LLMRec model loss in epoch 6/11 iteration 28/36: 0.3636089265346527\n",
            "A-LLMRec model loss in epoch 6/11 iteration 29/36: 0.052370622754096985\n",
            "A-LLMRec model loss in epoch 6/11 iteration 30/36: 0.0931861624121666\n",
            "A-LLMRec model loss in epoch 6/11 iteration 31/36: 0.0719476044178009\n",
            "A-LLMRec model loss in epoch 6/11 iteration 32/36: 0.851359486579895\n",
            "A-LLMRec model loss in epoch 6/11 iteration 33/36: 0.4496611952781677\n",
            "A-LLMRec model loss in epoch 6/11 iteration 34/36: 0.15150326490402222\n",
            "A-LLMRec model loss in epoch 6/11 iteration 35/36: 0.737498939037323\n",
            "A-LLMRec model loss in epoch 6/11 iteration 36/36: 0.4712151288986206\n",
            " 60% 6/10 [11:21<07:34, 113.58s/it]A-LLMRec model loss in epoch 7/11 iteration 0/36: 1.4794789552688599\n",
            "A-LLMRec model loss in epoch 7/11 iteration 1/36: 0.3078276216983795\n",
            "A-LLMRec model loss in epoch 7/11 iteration 2/36: 0.40536704659461975\n",
            "A-LLMRec model loss in epoch 7/11 iteration 3/36: 0.4360695481300354\n",
            "A-LLMRec model loss in epoch 7/11 iteration 4/36: 0.2922920286655426\n",
            "A-LLMRec model loss in epoch 7/11 iteration 5/36: 0.035866450518369675\n",
            "A-LLMRec model loss in epoch 7/11 iteration 6/36: 0.40964603424072266\n",
            "A-LLMRec model loss in epoch 7/11 iteration 7/36: 0.2725493609905243\n",
            "A-LLMRec model loss in epoch 7/11 iteration 8/36: 0.467289537191391\n",
            "A-LLMRec model loss in epoch 7/11 iteration 9/36: 0.34201574325561523\n",
            "A-LLMRec model loss in epoch 7/11 iteration 10/36: 0.15690124034881592\n",
            "A-LLMRec model loss in epoch 7/11 iteration 11/36: 0.2537594437599182\n",
            "A-LLMRec model loss in epoch 7/11 iteration 12/36: 0.1746961921453476\n",
            "A-LLMRec model loss in epoch 7/11 iteration 13/36: 0.0782281905412674\n",
            "A-LLMRec model loss in epoch 7/11 iteration 14/36: 0.30439290404319763\n",
            "A-LLMRec model loss in epoch 7/11 iteration 15/36: 0.484314888715744\n",
            "A-LLMRec model loss in epoch 7/11 iteration 16/36: 0.37663164734840393\n",
            "A-LLMRec model loss in epoch 7/11 iteration 17/36: 0.1524394154548645\n",
            "A-LLMRec model loss in epoch 7/11 iteration 18/36: 0.24465204775333405\n",
            "A-LLMRec model loss in epoch 7/11 iteration 19/36: 0.3994135558605194\n",
            "A-LLMRec model loss in epoch 7/11 iteration 20/36: 0.4071825444698334\n",
            "A-LLMRec model loss in epoch 7/11 iteration 21/36: 0.09508170187473297\n",
            "A-LLMRec model loss in epoch 7/11 iteration 22/36: 0.03305479511618614\n",
            "A-LLMRec model loss in epoch 7/11 iteration 23/36: 0.1756555587053299\n",
            "A-LLMRec model loss in epoch 7/11 iteration 24/36: 0.33330774307250977\n",
            "A-LLMRec model loss in epoch 7/11 iteration 25/36: 0.23236672580242157\n",
            "A-LLMRec model loss in epoch 7/11 iteration 26/36: 0.1264549046754837\n",
            "A-LLMRec model loss in epoch 7/11 iteration 27/36: 0.2266724854707718\n",
            "A-LLMRec model loss in epoch 7/11 iteration 28/36: 0.06508376449346542\n",
            "A-LLMRec model loss in epoch 7/11 iteration 29/36: 0.3998144865036011\n",
            "A-LLMRec model loss in epoch 7/11 iteration 30/36: 0.1502748280763626\n",
            "A-LLMRec model loss in epoch 7/11 iteration 31/36: 0.24498198926448822\n",
            "A-LLMRec model loss in epoch 7/11 iteration 32/36: 0.42032045125961304\n",
            "A-LLMRec model loss in epoch 7/11 iteration 33/36: 0.10834401845932007\n",
            "A-LLMRec model loss in epoch 7/11 iteration 34/36: 0.3721860349178314\n",
            "A-LLMRec model loss in epoch 7/11 iteration 35/36: 0.15894408524036407\n",
            "A-LLMRec model loss in epoch 7/11 iteration 36/36: 0.08806522935628891\n",
            " 70% 7/10 [13:16<05:42, 114.05s/it]A-LLMRec model loss in epoch 8/11 iteration 0/36: 0.4634493589401245\n",
            "A-LLMRec model loss in epoch 8/11 iteration 1/36: 0.3606105446815491\n",
            "A-LLMRec model loss in epoch 8/11 iteration 2/36: 0.2585287094116211\n",
            "A-LLMRec model loss in epoch 8/11 iteration 3/36: 0.33812597393989563\n",
            "A-LLMRec model loss in epoch 8/11 iteration 4/36: 0.27458545565605164\n",
            "A-LLMRec model loss in epoch 8/11 iteration 5/36: 0.04484159126877785\n",
            "A-LLMRec model loss in epoch 8/11 iteration 6/36: 0.590164065361023\n",
            "A-LLMRec model loss in epoch 8/11 iteration 7/36: 0.3530017137527466\n",
            "A-LLMRec model loss in epoch 8/11 iteration 8/36: 0.3855355381965637\n",
            "A-LLMRec model loss in epoch 8/11 iteration 9/36: 0.19438643753528595\n",
            "A-LLMRec model loss in epoch 8/11 iteration 10/36: 0.2553499639034271\n",
            "A-LLMRec model loss in epoch 8/11 iteration 11/36: 0.397773802280426\n",
            "A-LLMRec model loss in epoch 8/11 iteration 12/36: 0.22003334760665894\n",
            "A-LLMRec model loss in epoch 8/11 iteration 13/36: 0.18489231169223785\n",
            "A-LLMRec model loss in epoch 8/11 iteration 14/36: 0.040325988084077835\n",
            "A-LLMRec model loss in epoch 8/11 iteration 15/36: 0.42507490515708923\n",
            "A-LLMRec model loss in epoch 8/11 iteration 16/36: 0.008510979823768139\n",
            "A-LLMRec model loss in epoch 8/11 iteration 17/36: 0.3177269399166107\n",
            "A-LLMRec model loss in epoch 8/11 iteration 18/36: 0.013014495372772217\n",
            "A-LLMRec model loss in epoch 8/11 iteration 19/36: 0.21861417591571808\n",
            "A-LLMRec model loss in epoch 8/11 iteration 20/36: 0.2870827913284302\n",
            "A-LLMRec model loss in epoch 8/11 iteration 21/36: 0.12259893864393234\n",
            "A-LLMRec model loss in epoch 8/11 iteration 22/36: 0.2748068571090698\n",
            "A-LLMRec model loss in epoch 8/11 iteration 23/36: 0.10155105590820312\n",
            "A-LLMRec model loss in epoch 8/11 iteration 24/36: 0.22657908499240875\n",
            "A-LLMRec model loss in epoch 8/11 iteration 25/36: 0.005026536528021097\n",
            "A-LLMRec model loss in epoch 8/11 iteration 26/36: 0.6490598320960999\n",
            "A-LLMRec model loss in epoch 8/11 iteration 27/36: 0.07382148504257202\n",
            "A-LLMRec model loss in epoch 8/11 iteration 28/36: 0.5326934456825256\n",
            "A-LLMRec model loss in epoch 8/11 iteration 29/36: 0.2260953187942505\n",
            "A-LLMRec model loss in epoch 8/11 iteration 30/36: 0.39802828431129456\n",
            "A-LLMRec model loss in epoch 8/11 iteration 31/36: 0.3924861550331116\n",
            "A-LLMRec model loss in epoch 8/11 iteration 32/36: 0.42941904067993164\n",
            "A-LLMRec model loss in epoch 8/11 iteration 33/36: 0.04697057977318764\n",
            "A-LLMRec model loss in epoch 8/11 iteration 34/36: 0.3072984516620636\n",
            "A-LLMRec model loss in epoch 8/11 iteration 35/36: 0.2757182717323303\n",
            "A-LLMRec model loss in epoch 8/11 iteration 36/36: 0.12106816470623016\n",
            " 80% 8/10 [15:10<03:47, 113.97s/it]A-LLMRec model loss in epoch 9/11 iteration 0/36: 0.25748419761657715\n",
            "A-LLMRec model loss in epoch 9/11 iteration 1/36: 0.4068933129310608\n",
            "A-LLMRec model loss in epoch 9/11 iteration 2/36: 0.4757942855358124\n",
            "A-LLMRec model loss in epoch 9/11 iteration 3/36: 0.23552076518535614\n",
            "A-LLMRec model loss in epoch 9/11 iteration 4/36: 0.22868384420871735\n",
            "A-LLMRec model loss in epoch 9/11 iteration 5/36: 0.4634450376033783\n",
            "A-LLMRec model loss in epoch 9/11 iteration 6/36: 0.3072004020214081\n",
            "A-LLMRec model loss in epoch 9/11 iteration 7/36: 0.18162541091442108\n",
            "A-LLMRec model loss in epoch 9/11 iteration 8/36: 0.24645589292049408\n",
            "A-LLMRec model loss in epoch 9/11 iteration 9/36: 0.3698165714740753\n",
            "A-LLMRec model loss in epoch 9/11 iteration 10/36: 0.10318818688392639\n",
            "A-LLMRec model loss in epoch 9/11 iteration 11/36: 0.629901111125946\n",
            "A-LLMRec model loss in epoch 9/11 iteration 12/36: 0.19778311252593994\n",
            "A-LLMRec model loss in epoch 9/11 iteration 13/36: 0.2287278175354004\n",
            "A-LLMRec model loss in epoch 9/11 iteration 14/36: 0.40021151304244995\n",
            "A-LLMRec model loss in epoch 9/11 iteration 15/36: 0.2278483659029007\n",
            "A-LLMRec model loss in epoch 9/11 iteration 16/36: 0.30453410744667053\n",
            "A-LLMRec model loss in epoch 9/11 iteration 17/36: 0.2625974714756012\n",
            "A-LLMRec model loss in epoch 9/11 iteration 18/36: 0.518382728099823\n",
            "A-LLMRec model loss in epoch 9/11 iteration 19/36: 0.23331958055496216\n",
            "A-LLMRec model loss in epoch 9/11 iteration 20/36: 0.2862280607223511\n",
            "A-LLMRec model loss in epoch 9/11 iteration 21/36: 0.12306763976812363\n",
            "A-LLMRec model loss in epoch 9/11 iteration 22/36: 0.008390344679355621\n",
            "A-LLMRec model loss in epoch 9/11 iteration 23/36: 0.5788155198097229\n",
            "A-LLMRec model loss in epoch 9/11 iteration 24/36: 0.13759805262088776\n",
            "A-LLMRec model loss in epoch 9/11 iteration 25/36: 0.3970687985420227\n",
            "A-LLMRec model loss in epoch 9/11 iteration 26/36: 0.0477423369884491\n",
            "A-LLMRec model loss in epoch 9/11 iteration 27/36: 0.44339290261268616\n",
            "A-LLMRec model loss in epoch 9/11 iteration 28/36: 0.2360207736492157\n",
            "A-LLMRec model loss in epoch 9/11 iteration 29/36: 0.22636663913726807\n",
            "A-LLMRec model loss in epoch 9/11 iteration 30/36: 0.14652130007743835\n",
            "A-LLMRec model loss in epoch 9/11 iteration 31/36: 0.6047228574752808\n",
            "A-LLMRec model loss in epoch 9/11 iteration 32/36: 0.10688640177249908\n",
            "A-LLMRec model loss in epoch 9/11 iteration 33/36: 0.1837858110666275\n",
            "A-LLMRec model loss in epoch 9/11 iteration 34/36: 0.3090085983276367\n",
            "A-LLMRec model loss in epoch 9/11 iteration 35/36: 0.18417035043239594\n",
            "A-LLMRec model loss in epoch 9/11 iteration 36/36: 0.40916499495506287\n",
            " 90% 9/10 [17:04<01:54, 114.01s/it]A-LLMRec model loss in epoch 10/11 iteration 0/36: 0.056733388453722\n",
            "A-LLMRec model loss in epoch 10/11 iteration 1/36: 0.4690847098827362\n",
            "A-LLMRec model loss in epoch 10/11 iteration 2/36: 0.3550476133823395\n",
            "A-LLMRec model loss in epoch 10/11 iteration 3/36: 0.20473672449588776\n",
            "A-LLMRec model loss in epoch 10/11 iteration 4/36: 0.30826812982559204\n",
            "A-LLMRec model loss in epoch 10/11 iteration 5/36: 0.5421098470687866\n",
            "A-LLMRec model loss in epoch 10/11 iteration 6/36: 0.2151811718940735\n",
            "A-LLMRec model loss in epoch 10/11 iteration 7/36: 0.10182201862335205\n",
            "A-LLMRec model loss in epoch 10/11 iteration 8/36: 0.004508594516664743\n",
            "A-LLMRec model loss in epoch 10/11 iteration 9/36: 0.3419073820114136\n",
            "A-LLMRec model loss in epoch 10/11 iteration 10/36: 0.4757857620716095\n",
            "A-LLMRec model loss in epoch 10/11 iteration 11/36: 0.30630069971084595\n",
            "A-LLMRec model loss in epoch 10/11 iteration 12/36: 0.267569899559021\n",
            "A-LLMRec model loss in epoch 10/11 iteration 13/36: 0.4449855089187622\n",
            "A-LLMRec model loss in epoch 10/11 iteration 14/36: 0.6006671190261841\n",
            "A-LLMRec model loss in epoch 10/11 iteration 15/36: 0.43872424960136414\n",
            "A-LLMRec model loss in epoch 10/11 iteration 16/36: 0.19829468429088593\n",
            "A-LLMRec model loss in epoch 10/11 iteration 17/36: 0.252591997385025\n",
            "A-LLMRec model loss in epoch 10/11 iteration 18/36: 0.37987518310546875\n",
            "A-LLMRec model loss in epoch 10/11 iteration 19/36: 0.29799842834472656\n",
            "A-LLMRec model loss in epoch 10/11 iteration 20/36: 0.03001539036631584\n",
            "A-LLMRec model loss in epoch 10/11 iteration 21/36: 0.12228117883205414\n",
            "A-LLMRec model loss in epoch 10/11 iteration 22/36: 0.05574311688542366\n",
            "A-LLMRec model loss in epoch 10/11 iteration 23/36: 0.23771131038665771\n",
            "A-LLMRec model loss in epoch 10/11 iteration 24/36: 0.4006882607936859\n",
            "A-LLMRec model loss in epoch 10/11 iteration 25/36: 0.09229171276092529\n",
            "A-LLMRec model loss in epoch 10/11 iteration 26/36: 0.06219017878174782\n",
            "A-LLMRec model loss in epoch 10/11 iteration 27/36: 0.3168964385986328\n",
            "A-LLMRec model loss in epoch 10/11 iteration 28/36: 0.5554161667823792\n",
            "A-LLMRec model loss in epoch 10/11 iteration 29/36: 0.18184436857700348\n",
            "A-LLMRec model loss in epoch 10/11 iteration 30/36: 0.43604984879493713\n",
            "A-LLMRec model loss in epoch 10/11 iteration 31/36: 0.07714647799730301\n",
            "A-LLMRec model loss in epoch 10/11 iteration 32/36: 0.2847418785095215\n",
            "A-LLMRec model loss in epoch 10/11 iteration 33/36: 0.5609778165817261\n",
            "A-LLMRec model loss in epoch 10/11 iteration 34/36: 0.36266809701919556\n",
            "A-LLMRec model loss in epoch 10/11 iteration 35/36: 0.07771811634302139\n",
            "A-LLMRec model loss in epoch 10/11 iteration 36/36: 0.4870758652687073\n",
            "100% 10/10 [18:57<00:00, 113.71s/it]\n",
            "phase2 train time : 1137.0670919418335\n"
          ]
        }
      ],
      "source": [
        "!python main.py --pretrain_stage2 --rec_pre_trained_data Movies_and_TV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_ZAsoEt4uWa",
        "outputId": "f2568d2e-2b56-4118-a101-1f333e644eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-19 21:43:42.660058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-19 21:43:42.683067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-19 21:43:42.688974: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-19 21:43:42.702922: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-19 21:43:43.890172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "A-LLMRec start inference\n",
            "\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/recsys_model.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  kwargs, checkpoint = torch.load(pth_file_path[0], map_location=\"cpu\")\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Loading checkpoint shards: 100% 2/2 [01:02<00:00, 31.44s/it]\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/a_llmrec_model.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mlp = torch.load(out_dir + 'mlp.pt', map_location = args.device)\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/a_llmrec_model.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  log_emb_proj_dict = torch.load(out_dir + 'log_proj.pt', map_location = args.device)\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/a_llmrec_model.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  item_emb_proj_dict = torch.load(out_dir + 'item_proj.pt', map_location = args.device)\n",
            "user num: 73 item num: 67\n",
            "average sequence length: 4.47\n",
            "Initializing with num_user: 73\n",
            "/content/allmrec_CrossModal/A-LLMRec/models/a_llmrec_model.py:387: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        }
      ],
      "source": [
        "!python main.py --inference --rec_pre_trained_data Movies_and_TV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AVWJOZhLYTlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7ed297-633d-4f6b-f228-c5a27a44ea95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41 41\n",
            "41\n",
            "ndcg at 1: 0.17073170731707318\n",
            "hit at 1: 0.17073170731707318\n"
          ]
        }
      ],
      "source": [
        "!python eval.py\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}